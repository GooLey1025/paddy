train:
  batch_size: 32, 64
  patience: 20
  # agc_clip: 0.1
  num_gpu: 1
  train_epochs_max: 100
  
  loss: "mse" # default poisson. Options: mse; bce; poisson_mn; poisson_kl; mse_udot; poisson。
  ### below are for poisson_mn loss. ###
  #spec_weight: 1 # default 1. 较高的值会使模型更关注预测特定位置的准确性而非总体趋势
  #total_weight: 1 # default 1. 控制模型对总表达量与位置分布的平衡,增大模型更关注总表达量的准确预测，减小模型更关注位置分布的准确预测
  #weight_range: 1 # default 1. 当 >1 时，中心位置的权重会高于边缘位置。为5时，表示中心位置权重是边缘的5倍。
  #weight_exp: 1 # default 1. 权重衰减函数的指数，控制从中心到边缘权重下降的陡峭程度。较小的值使权重平缓变化，较大的值使中心高权重区域更集中。
  
  ### below are for optimizer. When not using cyclical1, only learning_rate is used.
  optimizer: "adam" # default sgd. Options: adam; adamw; sgd; momentum.
  learning_rate: 0.001,0.0001,0.00001 # default 0.01.
  adam_beta1: 0.90 # default 0.9.
  adam_beta2: 0.999 # default 0.999.

# 以下四个参数同时存在时启用循环学习率。
  #initial_learning_rate: 0.01 # default 0.01, when using cyclical1. 
  #maximal_learning_rate: 0.01 # default 0.01, when using cyclical1. 
  #final_learning_rate: 0.01 # default 0.01, when using cyclical1. 
  #train_epochs_cycle1: 10 # default 10, when using cyclical1. 

  # below are for AdamW optimizer.
  #weight_decay: 0 # default 0

  # below are for SGD/momentum optimizer.
  #monmentum: 0.99 # default 0.99.

  # below are parameters for learning rate decay.
  #decay_steps: 100000 # steps of decay，enable exponential decay when setting this.
  decay_rate: 0.96 # default 0.96.

  # below are parameters for warmup.
  # warmup_steps: 1000 # steps of warmup, enable warmup when setting this.

  # below are parameters for gradient clipping. Useful for preventing gradient explosion.
  #clip_norm: 1.0 # default 1.0. # 每个梯度的最大范数 [默认: null]
  #global_clipnorm: 5.0 # 所有梯度的全局最大范数 [默认: null]
  #agc_clip: 0.01 # 自适应梯度裁剪值 [默认: null]

model:
  num_bins: 1024
  num_tracks: 106
  activation: "gelu"
  norm_type: "batch"
  bn_momentum: 0.9
  kernel_initializer: "lecun_normal"
  l2_scale: 2.0e-8
  trunk:
    - block_name: "conv_dna"
      filters: 256
      kernel_size: 15,10
      norm_type: "null"
      activation: "linear"
      pool_size: 2
    - block_name: "res_tower"
      filters_init: 312
      filters_end: 512
      divisible_by: 32
      kernel_size: 15
      num_convs: 1
      pool_size: 2
      repeat: 4
    # - block_name: "dense_position"
    #   position_embedding_type: "learnable"
    #   hidden_dim: 512
    #   position_dropout: 0.1
    #   l2_scale: 0.01
    #   kernel_initializer: "he_normal"
    - block_name: "transformer"
      heads: 8
      key_size: 64 # The size of each key and query embedding per head.
      dropout: 0, 0.1, 0.2
      mha_l2_scale: 0 # default 0, 0意味着对权重不使用惩罚项。 
      mha_initializer: "he_normal" # 使用正态分布初始化权重，适合ReLU 激活函数
      attention_dropout: 0
      position_dropout: 0
    - block_name: "global_pool"
      pool_type: "attention"
      pool_axis: 1
    # - block_name: "Dense"
    #   units: 768
    #   activation: "relu"
    #   kernel_initializer: "he_normal"
    # - block_name: "Dense"
    #   units: 312
    #   activation: "relu"
    #   kernel_initializer: "he_normal"

  head_rice:
    - block_name: "dense_head"
      # hidden_units: 128 # default None. If setting, will add a dense layer with this number of units before the final output layer.
      units: 23 # number of targets
      activation: "softplus"
  # adapter: "houlsby" 
  