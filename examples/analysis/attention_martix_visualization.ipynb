{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 13:20:08.964296: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-12 13:20:09.105309: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-12 13:20:09.105409: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-12 13:20:09.131937: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-12 13:20:09.193826: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-12 13:20:09.655182: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-05-12 13:20:10.324681: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-12 13:20:10.619186: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-12 13:20:10.621664: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import baskerville.seqnn as seqnn\n",
    "\n",
    "targets_file = '/home/gl/projects/Borzoi/borzoi/examples/targets_gtex.txt'\n",
    "targets_df = pd.read_csv(targets_file, index_col=0, sep='\\t')\n",
    "target_index = targets_df.index\n",
    "\n",
    "\n",
    "params_file = \"/home/gl/projects/Borzoi/borzoi/examples/params_pred.json\"\n",
    "\n",
    "with open(params_file) as params_open :\n",
    "    params = json.load(params_open)\n",
    "    \n",
    "    params_model = params['model']\n",
    "    params_train = params['train']\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 13:20:37.813499: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-12 13:20:37.816496: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-12 13:20:37.817848: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-12 13:20:37.893006: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-12 13:20:37.893942: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-12 13:20:37.894780: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:236] Using CUDA malloc Async allocator for GPU: 0\n",
      "2025-05-12 13:20:37.895061: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-12 13:20:37.895941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9788 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "n_folds = 4\n",
    "SEQUENCE_LENGTH = 524288\n",
    "models = []\n",
    "\n",
    "for fold_ix in range(n_folds) :\n",
    "    model_file = f'/home/gl/projects/Borzoi/borzoi/examples/saved_models/f3c{fold_ix}/train/model0_best.h5'\n",
    "    seqnn_model = seqnn.SeqNN(params_model)\n",
    "    seqnn_model.restore(model_file, 0, trunk=False)\n",
    "    seqnn_model.build_ensemble(True, [0])\n",
    "    models.append(seqnn_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(gene_pr) = 1077833\n",
      "len(polya_pr) = 92482\n",
      "[Final] len(gene_pr) = 1170315\n"
     ]
    }
   ],
   "source": [
    "import pyranges as pr\n",
    "\n",
    "# load gff file\n",
    "#exons_gff_file = '/home/gl/projects/Borzoi/borzoi/examples/hg38/genes/gencode41/gencode41_basic_nort_protein_exons.gff'\n",
    "polya_gff_file = '/home/gl/projects/Borzoi/borzoi/examples/hg38/genes/gencode41/gencode.v41.polyAs.gff3'\n",
    "gene_gff_file = '/home/gl/projects/Borzoi/borzoi/examples/hg38/genes/gencode41/gencode.v41.basic.annotation.gff3'\n",
    "\n",
    "\n",
    "#exons_pr = pr.read_gff3(exons_gff_file)\n",
    "#exons_pr = exons_pr[exons_pr.Feature == 'exon']\n",
    "#exons_pr = exons_pr.drop('Source')\n",
    "#exons_pr = exons_pr.drop('Score')\n",
    "\n",
    "#print(\"len(exons_pr) = \" + str(len(exons_pr)))\n",
    "\n",
    "gene_pr = pr.read_gff3(gene_gff_file)\n",
    "\n",
    "gene_pr = gene_pr[gene_pr.Feature.isin(['gene', 'exon', 'five_prime_UTR', 'three_prime_UTR'])]\n",
    "\n",
    "gene_pr = gene_pr.drop('Source')\n",
    "gene_pr = gene_pr.drop('Score')\n",
    "gene_pr = gene_pr.drop('Frame')\n",
    "gene_pr = gene_pr.drop('ID')\n",
    "gene_pr = gene_pr.drop('hgnc_id')\n",
    "gene_pr = gene_pr.drop('havana_gene')\n",
    "gene_pr = gene_pr.drop('Parent')\n",
    "gene_pr = gene_pr.drop('transcript_support_level')\n",
    "gene_pr = gene_pr.drop('tag')\n",
    "gene_pr = gene_pr.drop('havana_transcript')\n",
    "gene_pr = gene_pr.drop('ont')\n",
    "gene_pr = gene_pr.drop('protein_id')\n",
    "gene_pr = gene_pr.drop('ccdsid')\n",
    "gene_pr = gene_pr.drop('artif_dupl')\n",
    "gene_pr = gene_pr.drop('level')\n",
    "\n",
    "print(\"len(gene_pr) = \" + str(len(gene_pr)))\n",
    "\n",
    "\n",
    "polya_pr = pr.read_gff3(polya_gff_file)\n",
    "\n",
    "polya_pr = polya_pr[polya_pr.Feature.isin(['polyA_signal', 'polyA_site'])]\n",
    "\n",
    "polya_pr = polya_pr.drop('Source')\n",
    "polya_pr = polya_pr.drop('Score')\n",
    "polya_pr = polya_pr.drop('Frame')\n",
    "polya_pr = polya_pr.drop('ID')\n",
    "polya_pr = polya_pr.drop('gene_type')\n",
    "polya_pr = polya_pr.drop('gene_name')\n",
    "polya_pr = polya_pr.drop('transcript_type')\n",
    "polya_pr = polya_pr.drop('transcript_name')\n",
    "polya_pr = polya_pr.drop('level')\n",
    "\n",
    "print(\"len(polya_pr) = \" + str(len(polya_pr)))\n",
    "\n",
    "#Concatenate annotations\n",
    "gene_pr = pr.concat([gene_pr, polya_pr])\n",
    "\n",
    "print(\"[Final] len(gene_pr) = \" + str(len(gene_pr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib.text import TextPath\n",
    "from matplotlib.patches import PathPatch, Rectangle\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "import gc\n",
    "from baskerville import dna\n",
    "\n",
    "#Function to get a one-hot coded sequence pattern\n",
    "def make_seq_1hot(genome_open, chrm, start, end, seq_len):\n",
    "    if start < 0:\n",
    "        seq_dna = 'N'*(-start) + genome_open.fetch(chrm, 0, end)\n",
    "    else:\n",
    "        seq_dna = genome_open.fetch(chrm, start, end)\n",
    "    \n",
    "    #Extend to full length (pad)\n",
    "    if len(seq_dna) < seq_len:\n",
    "        seq_dna += 'N'*(seq_len-len(seq_dna))\n",
    "    \n",
    "    seq_1hot = dna.dna_1hot(seq_dna)\n",
    "    \n",
    "    return seq_1hot\n",
    "\n",
    "#Function to inject into model definition to get attention scores\n",
    "def _get_attention_weights(self, inputs, training=False) :\n",
    "    \n",
    "    #Initialise the projection layers\n",
    "    embedding_size = self._value_size * self._num_heads\n",
    "    seq_len = inputs.shape[1]\n",
    "\n",
    "    #Compute q, k and v as multi-headed projections of the inputs\n",
    "    q = self._multihead_output(self._q_layer, inputs)  # [B, H, T, K]\n",
    "    k = self._multihead_output(self._k_layer, inputs)  # [B, H, T, K]\n",
    "    v = self._multihead_output(self._v_layer, inputs)  # [B, H, T, V]\n",
    "\n",
    "    #Scale the query by the square-root of key size\n",
    "    if self._scaling :\n",
    "        q *= self._key_size**-0.5\n",
    "\n",
    "    #[B, H, T', T]\n",
    "    content_logits = tf.matmul(q + self._r_w_bias, k, transpose_b=True)\n",
    "\n",
    "    if self._num_position_features == 0 :\n",
    "        logits = content_logits\n",
    "    else :\n",
    "        #Project positions to form relative keys.\n",
    "        distances = tf.range(-seq_len + 1, seq_len, dtype=tf.float32)[tf.newaxis]\n",
    "        positional_encodings = basenji.layers.positional_features(\n",
    "              positions=distances,\n",
    "              feature_size=self._num_position_features,\n",
    "              seq_length=seq_len,\n",
    "              symmetric=self._relative_position_symmetric)\n",
    "        #[1, 2T-1, Cr]\n",
    "      \n",
    "        if training :\n",
    "            positional_encodings = tf.nn.dropout(\n",
    "                positional_encodings, rate=self._positional_dropout_rate)\n",
    "\n",
    "        #[1, H, 2T-1, K]\n",
    "        r_k = self._multihead_output(self._r_k_layer, positional_encodings)\n",
    "\n",
    "        #Add shifted relative logits to content logits.\n",
    "        if self._content_position_bias :\n",
    "            #[B, H, T', 2T-1]\n",
    "            relative_logits = tf.matmul(q + self._r_r_bias, r_k, transpose_b=True)\n",
    "        else :\n",
    "            #[1, H, 1, 2T-1]\n",
    "            relative_logits = tf.matmul(self._r_r_bias, r_k, transpose_b=True)\n",
    "            #[1, H, T', 2T-1]\n",
    "            relative_logits = tf.broadcast_to(relative_logits, shape=(1, self._num_heads, seq_len, 2*seq_len-1))\n",
    "\n",
    "        #[B, H, T', T]\n",
    "        relative_logits = basenji.layers.relative_shift(relative_logits)\n",
    "        logits = content_logits + relative_logits\n",
    "\n",
    "    #Apply Softmax across length\n",
    "    weights = tf.nn.softmax(logits)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "#Function to create a new Keras model, outputting a specific attention layer's scores\n",
    "def get_attention_model(seqnn_model, layer_ix=0, inital_offset=30, offset=11) :\n",
    "\n",
    "    #Create new model object. \n",
    "    attention_model = tf.keras.Model(\n",
    "        seqnn_model.model.layers[1].inputs, # modified: inputs -> input\n",
    "        _get_attention_weights(\n",
    "            seqnn_model.model.layers[1].layers[inital_offset + offset * layer_ix],\n",
    "            seqnn_model.model.layers[1].layers[inital_offset + offset * layer_ix - 1].output,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return attention_model\n",
    "\n",
    "# def get_model_info(seqnn_model):\n",
    "#     # loop through all layers and print the layer name, input shape, output shape\n",
    "#     for layer in seqnn_model.model.layers:\n",
    "#         print(f\"{layer.name}: {layer.input_shape} -> {layer.output_shape}\")\n",
    "#     with open('all_layers.txt', 'w') as f:\n",
    "#         for layer in seqnn_model.model.layers:\n",
    "#             f.write(f\"{layer.name}: {layer.input_shape} -> {layer.output_shape}\\n\")\n",
    "\n",
    "\n",
    "#Function to predict tracks and attention scores\n",
    "def predict_tracks_and_attention_scores(models, sequence_one_hot, track_scale=1., track_transform=1., clip_soft=None, n_layers=8, score_rc=True) :\n",
    "    \n",
    "    attention_scores = []\n",
    "    predicted_tracks = []\n",
    "    \n",
    "    #Loop over folds\n",
    "    for fold_ix in range(n_folds) :\n",
    "\n",
    "        attention_scores_for_fold = []\n",
    "\n",
    "        yh = models[fold_ix](sequence_one_hot[None, ...])[:, None, ...].astype('float32')\n",
    "        \n",
    "        #Undo scale\n",
    "        yh /= track_scale\n",
    "\n",
    "        #Undo soft_clip\n",
    "        if clip_soft is not None :\n",
    "            yh_unclipped = (yh - clip_soft)**2 + clip_soft\n",
    "            unclip_mask_h = (yh > clip_soft)\n",
    "\n",
    "            yh[unclip_mask_h] = yh_unclipped[unclip_mask_h]\n",
    "\n",
    "        #Undo sqrt\n",
    "        yh = yh**(1. / track_transform)\n",
    "\n",
    "        #Aggregate over tracks (average)\n",
    "        ##yh = np.mean(yh, axis=-1)\n",
    "\n",
    "        predicted_tracks.append(yh)\n",
    "\n",
    "        #Loop over layers\n",
    "        for layer_ix in range(n_layers) :\n",
    "\n",
    "            #Get attention score model and make predictions\n",
    "            attention_model = get_attention_model(models[fold_ix], layer_ix=layer_ix)\n",
    "            att_scores = attention_model.predict(x=[sequence_one_hot[None, ...]], batch_size=1)\n",
    "            \n",
    "            #Optionally reverse-complement\n",
    "            if score_rc :\n",
    "                att_scores_rc = attention_model.predict(x=[sequence_one_hot[None, ::-1, ::-1]], batch_size=1)\n",
    "                att_scores = (att_scores + att_scores_rc[..., ::-1, ::-1]) / 2.\n",
    "\n",
    "            attention_scores_for_fold.append(att_scores[:, None, None, ...])\n",
    "\n",
    "            attention_model = None\n",
    "            gc.collect()\n",
    "\n",
    "        #Concatenate\n",
    "        attention_scores_for_fold = np.concatenate(attention_scores_for_fold, axis=2)\n",
    "        attention_scores.append(attention_scores_for_fold)\n",
    "\n",
    "    #Concatenate\n",
    "    predicted_tracks = np.concatenate(predicted_tracks, axis=1)\n",
    "    attention_scores = np.concatenate(attention_scores, axis=1)\n",
    "\n",
    "    print(\"predicted_tracks.shape = \" + str(predicted_tracks.shape))\n",
    "    print(\"attention_scores.shape = \" + str(attention_scores.shape))\n",
    "    \n",
    "    return predicted_tracks, attention_scores\n",
    "\n",
    "#Function to predict tracks\n",
    "def predict_tracks(models, sequence_one_hot, track_scale=1., track_transform=1., clip_soft=None) :\n",
    "    \n",
    "    predicted_tracks = []\n",
    "    \n",
    "    #Loop over folds\n",
    "    for fold_ix in range(n_folds) :\n",
    "\n",
    "        yh = models[fold_ix](sequence_one_hot[None, ...])[:, None, ...].astype('float32')\n",
    "        \n",
    "        #Undo scale\n",
    "        yh /= track_scale\n",
    "\n",
    "        #Undo soft_clip\n",
    "        if clip_soft is not None :\n",
    "            yh_unclipped = (yh - clip_soft)**2 + clip_soft\n",
    "            unclip_mask_h = (yh > clip_soft)\n",
    "\n",
    "            yh[unclip_mask_h] = yh_unclipped[unclip_mask_h]\n",
    "\n",
    "        #Undo sqrt\n",
    "        yh = yh**(1. / track_transform)\n",
    "\n",
    "        predicted_tracks.append(yh)\n",
    "\n",
    "    #Concatenate\n",
    "    predicted_tracks = np.concatenate(predicted_tracks, axis=1)\n",
    "    print(\"predicted_tracks.shape = \" + str(predicted_tracks.shape))\n",
    "    \n",
    "    return predicted_tracks\n",
    "\n",
    "#Helper function to get (padded) one-hot and annotated sub-dataframe\n",
    "def process_sequence(chrom, start, end) :\n",
    "    \n",
    "    seq_len = end - start\n",
    "\n",
    "    #Pad sequence to input window size\n",
    "    start -= (SEQUENCE_LENGTH - seq_len) // 2\n",
    "    end += (SEQUENCE_LENGTH - seq_len) // 2\n",
    "\n",
    "    annotation_df = gene_pr.df.query(\"Chromosome == '\" + chrom + \"' and ((End >= \" + str(int(start)) + \" and End < \" + str(int(end)) + \") or (Start >= \" + str(int(start)) + \" and Start < \" + str(int(end)) + \"))\")\n",
    "\n",
    "    #Get one-hot\n",
    "    sequence_one_hot = make_seq_1hot(fasta_open, chrom, start, end, seq_len)\n",
    "    \n",
    "    return sequence_one_hot.astype('float32'), annotation_df\n",
    "\n",
    "#Function for visualizing attention scores and predicted tracks\n",
    "def plot_attention_score(predicted_tracks, attention_scores, chrom='chr1', start=0, end=1024, track_crop=16, track_pool=1, use_gaussian=False, gaussian_sigma=8, gaussian_truncate=2, plot_start=0, plot_end=1024, save_suffix='', vmin=0.0001, vmax=0.005, highlight_area=False, annotate_features=[], highlight_start=0, highlight_end=1, highlight_start_y=None, highlight_end_y=None, example_ix=0, track_scale_qtl=0.95, track_scale_val=None, track_scale=0.02, track_clip=0.08, fold_index=[0, 1, 2, 3], layer_index=[5, 6], head_index=[0, 1, 2, 3, 4, 5, 6, 7], figsize=(8, 8), fig_dpi=600, save_figs=False) :\n",
    "\n",
    "    #Average over tracks\n",
    "    yh = np.mean(predicted_tracks, axis=1)[example_ix, ...]\n",
    "    \n",
    "    #Pool track bins\n",
    "    if track_pool > 1 :\n",
    "        yh = np.mean(np.reshape(yh, (yh.shape[0] // track_pool, track_pool)), axis=-1)\n",
    "    \n",
    "    track_crop_pooled = track_crop // 4\n",
    "    \n",
    "    #Calculate track-to-attention ratio\n",
    "    track_ratio = yh.shape[0] // (4096 - 2 * track_crop_pooled)\n",
    "    \n",
    "    if track_scale_val is None :\n",
    "        track_scale_val = round(np.quantile(yh, q=track_scale_qtl), 4)\n",
    "        print(\"track_scale_val = \" + str(track_scale_val))\n",
    "    \n",
    "    #Zero-pad\n",
    "    yh = np.concatenate([np.zeros(track_crop_pooled * track_ratio), yh / track_scale_val, np.zeros(track_crop_pooled * track_ratio)], axis=0)\n",
    "    \n",
    "    #Average attention map\n",
    "    att = np.mean(\n",
    "        np.mean(\n",
    "            np.mean(\n",
    "                attention_scores[:, fold_index, ...]\n",
    "                , axis=1\n",
    "            )[:, layer_index, ...]\n",
    "            , axis=1\n",
    "        )[:, head_index, ...]\n",
    "        , axis=1\n",
    "    )[example_ix, ...]\n",
    "    \n",
    "    \n",
    "    #Optionally apply gaussian smoothing to attention map\n",
    "    if use_gaussian :\n",
    "        att = gaussian_filter(att, sigma=gaussian_sigma, truncate=gaussian_truncate)\n",
    "\n",
    "    plot_start_bin = (plot_start - start) // 128\n",
    "    plot_end_bin = (plot_end - start) // 128\n",
    "\n",
    "    track_scale = int(track_scale * (plot_end_bin - plot_start_bin))\n",
    "    track_clip = int(track_clip * (plot_end_bin - plot_start_bin))\n",
    "\n",
    "    highlight_start_x_bin = (highlight_start - start) // 128\n",
    "    highlight_end_x_bin = (highlight_end - start) // 128\n",
    "    \n",
    "    if highlight_start_y is not None and highlight_end_y is not None :\n",
    "        highlight_start_y_bin = (highlight_start_y - start) // 128\n",
    "        highlight_end_y_bin = (highlight_end_y - start) // 128\n",
    "    else :\n",
    "        highlight_start_y_bin = (highlight_start - start) // 128\n",
    "        highlight_end_y_bin = (highlight_end - start) // 128\n",
    "\n",
    "    att[att < vmin] = 0.\n",
    "\n",
    "    f = plt.figure(figsize=figsize)\n",
    "\n",
    "    plt.imshow(att, cmap='hot', vmin=0., vmax=vmax, aspect='equal')\n",
    "\n",
    "    #Draw highlighted rectangle\n",
    "    if highlight_area :\n",
    "        rect = patches.Rectangle(\n",
    "            (highlight_start_x_bin, highlight_start_y_bin),\n",
    "            np.minimum(highlight_end_x_bin-highlight_start_x_bin, 4096-highlight_start_x_bin),\n",
    "            np.minimum(highlight_end_y_bin-highlight_start_y_bin, 4096-highlight_start_y_bin), linewidth=1, edgecolor='magenta', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "\n",
    "    #Draw annotation features\n",
    "    for z_order, annotate_dict in enumerate(annotate_features) :\n",
    "\n",
    "        feature_df = annotation_df.query(\"Feature == '\" + annotate_dict['feature'] + \"'\" + ((\" and \" + annotate_dict['filter_query']) if annotate_dict['filter_query'] is not None and annotate_dict['filter_query'] != \"\" else \"\"))\n",
    "\n",
    "        #Loop over features of the given type\n",
    "        for _, row in feature_df.iterrows() :\n",
    "\n",
    "            start_f = row['Start']\n",
    "            end_f = row['End']\n",
    "\n",
    "            #Plot features of a given length range only\n",
    "            if end_f - start_f < annotate_dict['min_len'] or end_f - start_f > annotate_dict['max_len'] :\n",
    "                continue\n",
    "\n",
    "            start_f_bin = (start_f - start) // 128\n",
    "            end_f_bin = (end_f - start) // 128\n",
    "\n",
    "            #Draw gene text annotation\n",
    "            if annotate_dict['feature'] == 'gene' and annotate_dict['annotate_text'] == True :\n",
    "                if start_f_bin > plot_start_bin and end_f_bin < plot_end_bin :\n",
    "                    plt.gca().text(start_f_bin, end_f_bin + 1, (\"<- \" if row['Strand'] == '-' else \"\") + row['gene_name'] + (\" ->\" if row['Strand'] == '+' else \"\"), ha=\"left\", va=\"bottom\", rotation=0, size=8, color=annotate_dict['color'], fontweight='bold')\n",
    "\n",
    "            #Draw box, line or dots to highlight feature\n",
    "            if annotate_dict['plot_type'] == 'box' :\n",
    "                rect = patches.Rectangle(\n",
    "                    (start_f_bin, start_f_bin),\n",
    "                    np.minimum(end_f_bin-start_f_bin, 4096-start_f_bin),\n",
    "                    np.minimum(end_f_bin-start_f_bin, 4096-start_f_bin), linewidth=2, edgecolor=annotate_dict['color'], facecolor='none')\n",
    "                plt.gca().add_patch(rect)\n",
    "            elif annotate_dict['plot_type'] == 'line' :\n",
    "                plt.plot([start_f_bin, end_f_bin], [start_f_bin, end_f_bin], linewidth=2, color=annotate_dict['color'], zorder=z_order)\n",
    "            elif annotate_dict['plot_type'] == 'dots' :\n",
    "                plt.scatter([start_f_bin], [start_f_bin], marker=annotate_dict['marker'], s=annotate_dict['size'], color=annotate_dict['color'], edgecolor='black', linewidth=0.5, zorder=z_order)\n",
    "                plt.scatter([end_f_bin], [end_f_bin], marker=annotate_dict['marker'], s=annotate_dict['size'], color=annotate_dict['color'], edgecolor='black', linewidth=0.5, zorder=z_order)\n",
    "\n",
    "    z_order = len(annotate_features)\n",
    "\n",
    "    #Plot track densities (y-axis)\n",
    "    plt.gca().fill_between(np.linspace(plot_start_bin, plot_end_bin+track_clip+1, num=yh[plot_start_bin*track_ratio:plot_end_bin*track_ratio].shape[0]+track_clip+1), track_clip + plot_end_bin + 1, y2=plot_end_bin, color='white', zorder=z_order)\n",
    "    plt.gca().fill_between(np.arange((plot_end_bin - plot_start_bin)*track_ratio)/track_ratio + plot_start_bin, np.clip(track_scale * yh[plot_start_bin*track_ratio:plot_end_bin*track_ratio], 0, track_clip) + plot_end_bin, y2=plot_end_bin, color='deepskyblue', zorder=z_order+1)\n",
    "    plt.plot(np.arange((plot_end_bin - plot_start_bin)*track_ratio)/track_ratio + plot_start_bin, np.clip(track_scale * yh[plot_start_bin*track_ratio:plot_end_bin*track_ratio], 0, track_clip) + plot_end_bin, linewidth=1, color='black', zorder=z_order+2)\n",
    "\n",
    "    #Plot densitites (x-axis)\n",
    "    plt.gca().fill_betweenx(np.linspace(plot_start_bin-track_clip, plot_end_bin, num=yh[plot_start_bin*track_ratio:plot_end_bin*track_ratio].shape[0]+track_clip), track_clip + plot_end_bin + 1, x2=plot_end_bin, color='white', zorder=z_order)\n",
    "    plt.gca().fill_betweenx(np.arange((plot_end_bin - plot_start_bin)*track_ratio)/track_ratio + plot_start_bin, np.clip(track_scale * yh[plot_start_bin*track_ratio:plot_end_bin*track_ratio], 0, track_clip) + plot_end_bin, x2=plot_end_bin, color='deepskyblue', zorder=z_order+1)\n",
    "    plt.plot(np.clip(track_scale * yh[plot_start_bin*track_ratio:plot_end_bin*track_ratio], 0, track_clip) + plot_end_bin, np.arange((plot_end_bin - plot_start_bin)*track_ratio)/track_ratio + plot_start_bin, linewidth=1, color='black', zorder=z_order+2)\n",
    "\n",
    "    #Draw bounding frames\n",
    "    plt.plot([track_crop_pooled, track_crop_pooled], [4096, 4096 + track_clip], color='red', linestyle='--', linewidth=1, zorder=z_order+3)\n",
    "    plt.plot([4096 - track_crop_pooled, 4096 - track_crop_pooled], [4096, 4096 + track_clip], color='red', linestyle='--', linewidth=1, zorder=z_order+3)\n",
    "\n",
    "    plt.plot([4096, 4096 + track_clip], [track_crop_pooled, track_crop_pooled], color='red', linestyle='--', linewidth=1, zorder=z_order+3)\n",
    "    plt.plot([4096, 4096 + track_clip], [4096 - track_crop_pooled, 4096 - track_crop_pooled], color='red', linestyle='--', linewidth=1, zorder=z_order+3)\n",
    "\n",
    "    plt.xlim(plot_start_bin, plot_end_bin+track_clip+1)\n",
    "    plt.ylim(plot_start_bin, plot_end_bin+track_clip+1)\n",
    "\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "\n",
    "    #Remove borders\n",
    "    for spine in plt.gca().spines.values():\n",
    "        spine.set_edgecolor('white')\n",
    "\n",
    "    plt.gca().set_xlabel(\"Attended on (\" + chrom + \":\" + str(plot_start) + \"-\" + str(plot_end) + \")  --->\", fontsize=12, loc='left')\n",
    "    plt.gca().set_ylabel(\"Attended by (\" + chrom + \":\" + str(plot_start) + \"-\" + str(plot_end) + \")  --->\", fontsize=12, loc='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_figs :\n",
    "        plt.savefig(\"attention_scores_\" + chrom + \"_\" + str(plot_start) + \"-\" + str(plot_end) + save_suffix + \".png\", dpi=fig_dpi)\n",
    "        plt.savefig(\"attention_scores_\" + chrom + \"_\" + str(plot_start) + \"-\" + str(plot_end) + save_suffix + \".eps\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#Helper function to plot ACGT letters at a given position\n",
    "def dna_letter_at(letter, x, y, yscale=1, ax=None, color=None, alpha=1.0):\n",
    "\n",
    "    fp = FontProperties(family=\"DejaVu Sans\", weight=\"bold\")\n",
    "    globscale = 1.35\n",
    "    LETTERS = {\t\"T\" : TextPath((-0.305, 0), \"T\", size=1, prop=fp),\n",
    "                \"G\" : TextPath((-0.384, 0), \"G\", size=1, prop=fp),\n",
    "                \"A\" : TextPath((-0.35, 0), \"A\", size=1, prop=fp),\n",
    "                \"C\" : TextPath((-0.366, 0), \"C\", size=1, prop=fp),\n",
    "                \"UP\" : TextPath((-0.488, 0), '$\\\\Uparrow$', size=1, prop=fp),\n",
    "                \"DN\" : TextPath((-0.488, 0), '$\\\\Downarrow$', size=1, prop=fp),\n",
    "                \"(\" : TextPath((-0.25, 0), \"(\", size=1, prop=fp),\n",
    "                \".\" : TextPath((-0.125, 0), \"-\", size=1, prop=fp),\n",
    "                \")\" : TextPath((-0.1, 0), \")\", size=1, prop=fp)}\n",
    "    COLOR_SCHEME = {'G': 'orange',#'orange', \n",
    "                    'A': 'green',#'red', \n",
    "                    'C': 'blue',#'blue', \n",
    "                    'T': 'red',#'darkgreen',\n",
    "                    'UP': 'green', \n",
    "                    'DN': 'red',\n",
    "                    '(': 'black',\n",
    "                    '.': 'black', \n",
    "                    ')': 'black'}\n",
    "\n",
    "\n",
    "    text = LETTERS[letter]\n",
    "\n",
    "    chosen_color = COLOR_SCHEME[letter]\n",
    "    if color is not None :\n",
    "        chosen_color = color\n",
    "\n",
    "    t = mpl.transforms.Affine2D().scale(1*globscale, yscale*globscale) + \\\n",
    "        mpl.transforms.Affine2D().translate(x,y) + ax.transData\n",
    "    p = PathPatch(text, lw=0, fc=chosen_color, alpha=alpha, transform=t)\n",
    "    if ax != None:\n",
    "        ax.add_artist(p)\n",
    "    return p\n",
    "\n",
    "#TF function to get average attention gradient\n",
    "def _attention_input_grad(input_sequence, model, att_start_x, att_end_x, att_start_y, att_end_y, subtract_avg) :\n",
    "\n",
    "    #Average slice of attention map\n",
    "    mean_attention = None\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_sequence)\n",
    "        mean_attention = tf.reduce_mean(model(input_sequence)[:, :, att_start_x:att_end_x, att_start_y:att_end_y], axis=(1, 2, 3))\n",
    "\n",
    "    #Calculate gradient\n",
    "    input_grad = tape.gradient(mean_attention, input_sequence)\n",
    "    \n",
    "    #Optionally mean-subtract\n",
    "    if subtract_avg :\n",
    "        input_grad = (input_grad - tf.reduce_mean(input_grad, axis=-1, keepdims=True))\n",
    "    else :\n",
    "        input_grad = input_grad\n",
    "\n",
    "    return input_grad\n",
    "\n",
    "#Function to compute attention gradient on list of sequences (with rc)\n",
    "def get_attention_gradient_w_rc(models, sequence_one_hots, att_start_x, att_end_x, att_start_y, att_end_y, subtract_avg=False, fold_index=[0, 1, 2, 3], layer_index=[5, 6], inital_offset=30, offset=11) :\n",
    "    \n",
    "    #Get gradients for fwd\n",
    "    att_grads = get_attention_gradient(models, sequence_one_hots, att_start_x, att_end_x, att_start_y, att_end_y, subtract_avg, fold_index, layer_index, inital_offset, offset)\n",
    "    \n",
    "    #Get rev sequences\n",
    "    sequence_one_hots_rc = [\n",
    "        sequence_one_hots[example_ix][::-1, ::-1] for example_ix in range(len(sequence_one_hots))\n",
    "    ]\n",
    "    \n",
    "    #Invert attention box\n",
    "    att_start_x = 4096 - att_start_x - 1\n",
    "    att_end_x = 4096 - att_end_x - 1\n",
    "    att_start_y = 4096 - att_start_y - 1\n",
    "    att_end_y = 4096 - att_end_y - 1\n",
    "    \n",
    "    #Get gradients for rev\n",
    "    att_grads_rc = get_attention_gradient(models, sequence_one_hots_rc, att_end_y, att_start_y, att_end_x, att_start_x, subtract_avg, fold_index, layer_index, inital_offset, offset)\n",
    "    \n",
    "    #Average fwd and rev\n",
    "    att_grads_avg = [\n",
    "        (att_grads[example_ix] + att_grads_rc[example_ix][::-1, ::-1]) / 2. for example_ix in range(len(sequence_one_hots))\n",
    "    ]\n",
    "    \n",
    "    return att_grads, att_grads_rc, att_grads_avg\n",
    "\n",
    "#Function to compute attention gradient on list of sequences\n",
    "def get_attention_gradient(models, sequence_one_hots, att_start_x, att_end_x, att_start_y, att_end_y, subtract_avg=False, fold_index=[0, 1, 2, 3], layer_index=[5, 6], inital_offset=30, offset=11) :\n",
    "    \n",
    "    att_grads = np.zeros((len(fold_index), len(layer_index), len(sequence_one_hots), 524288, 4))\n",
    "    \n",
    "    #Loop over folds\n",
    "    for fold_i, fold_ix in enumerate(fold_index) :\n",
    "        \n",
    "        seqnn_model = models[fold_ix]\n",
    "        \n",
    "        #Loop over layers\n",
    "        for layer_i, layer_ix in enumerate(layer_index) :\n",
    "\n",
    "            #Construct attention keras model\n",
    "            attention_model = tf.keras.Model(\n",
    "                seqnn_model.model.layers[1].inputs,\n",
    "                _get_attention_weights(\n",
    "                    seqnn_model.model.layers[1].layers[inital_offset + offset * layer_ix],\n",
    "                    seqnn_model.model.layers[1].layers[inital_offset + offset * layer_ix - 1].output,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            #Define new input tensor\n",
    "            input_sequence = tf.keras.layers.Input(shape=(524288, 4), name='sequence')\n",
    "\n",
    "            #Define lambda layer around attention gradient function\n",
    "            input_grad = tf.keras.layers.Lambda(lambda x: _attention_input_grad(x, attention_model, att_start_x, att_end_x, att_start_y, att_end_y, subtract_avg), name='inp_grad')(input_sequence)\n",
    "\n",
    "            #Create a new keras model that returns the attention gradient\n",
    "            grad_model = tf.keras.models.Model(input_sequence, input_grad)\n",
    "            \n",
    "            #Calculate gradient on CPU\n",
    "            with tf.device('/cpu:0') :\n",
    "                for example_ix in range(len(sequence_one_hots)) :\n",
    "                    att_grads[fold_i, layer_i, example_ix, ...] = sequence_one_hots[example_ix] * grad_model.predict(x=[sequence_one_hots[example_ix][None, ...]], batch_size=1, verbose=True)[0, ...]\n",
    "            \n",
    "            #Run garbage collection before next layer/fold\n",
    "            attention_model = None\n",
    "            gc.collect()\n",
    "    \n",
    "    #Average and input-gate\n",
    "    att_grads = np.mean(att_grads, axis=(0, 1))\n",
    "    att_grads = [\n",
    "        np.sum(att_grads[example_ix, ...], axis=-1, keepdims=True) * sequence_one_hots[example_ix] for example_ix in range(len(sequence_one_hots))\n",
    "    ]\n",
    "    \n",
    "    return att_grads\n",
    "\n",
    "#TF function to get gradient of predictions for a subset of positions and tracks (averaged across tracks)\n",
    "def _prediction_input_grad(input_sequence, model, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, track_index, track_scale, track_transform, clip_soft, use_mean, use_ratio, use_logodds, subtract_avg, prox_bin_index, dist_bin_index) :\n",
    "\n",
    "    mean_dist_prox_ratio = None\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_sequence)\n",
    "        \n",
    "        #Predict and gather selected output tracks\n",
    "        preds = tf.gather(model(input_sequence, training=False), tf.tile(tf.constant(np.array(track_index))[None, :], (tf.shape(input_sequence)[0], 1)), axis=2, batch_dims=1)\n",
    "        \n",
    "        #Undo scale\n",
    "        preds = preds / track_scale\n",
    "\n",
    "        #Undo soft_clip\n",
    "        if clip_soft is not None :\n",
    "            preds = tf.where(preds > clip_soft, (preds - clip_soft)**2 + clip_soft, preds)\n",
    "\n",
    "        #Undo sqrt\n",
    "        preds = preds**(1. / track_transform)\n",
    "      \n",
    "        #Aggregate over tracks (average)\n",
    "        pred = tf.reduce_mean(preds, axis=2)\n",
    "        \n",
    "        #Aggregate over positions and optionally compute ratios\n",
    "        if not use_mean :\n",
    "            #Get sum of distal coverage\n",
    "            if dist_bin_index is None :\n",
    "                mean_dist = tf.reduce_sum(pred[:, dist_bin_start:dist_bin_end], axis=1)\n",
    "            else :\n",
    "                mean_dist = tf.reduce_sum(tf.gather(pred, dist_bin_index, axis=1), axis=1)\n",
    "            \n",
    "            #Get sum of proximal coverage\n",
    "            if prox_bin_index is None :\n",
    "                mean_prox = tf.reduce_sum(pred[:, prox_bin_start:prox_bin_end], axis=1)\n",
    "            else :\n",
    "                mean_prox = tf.reduce_sum(tf.gather(pred, prox_bin_index, axis=1), axis=1)\n",
    "        else :\n",
    "            #Get mean distal coverage\n",
    "            if dist_bin_index is None :\n",
    "                mean_dist = tf.reduce_mean(pred[:, dist_bin_start:dist_bin_end], axis=1)\n",
    "            else :\n",
    "                mean_dist = tf.reduce_mean(tf.gather(pred, dist_bin_index, axis=1), axis=1)\n",
    "            \n",
    "            #Get mean proximal coverage\n",
    "            if prox_bin_index is None :\n",
    "                mean_prox = tf.reduce_mean(pred[:, prox_bin_start:prox_bin_end], axis=1)\n",
    "            else :\n",
    "                mean_prox = tf.reduce_mean(tf.gather(pred, prox_bin_index, axis=1), axis=1)\n",
    "        if not use_ratio :\n",
    "            #Get log-sum of coverage\n",
    "            mean_dist_prox_ratio = tf.math.log(mean_dist + 1e-6)\n",
    "        else :\n",
    "            #Get ratio (or log odds) of distal and proximal coverage samples\n",
    "            if not use_logodds :\n",
    "                mean_dist_prox_ratio = tf.math.log(mean_dist / mean_prox + 1e-6)\n",
    "            else :\n",
    "                mean_dist_prox_ratio = tf.math.log((mean_dist / mean_prox) / (1. - (mean_dist / mean_prox)) + 1e-6)\n",
    "\n",
    "    #Calculate gradient and optionally mean-subtract\n",
    "    input_grad = tape.gradient(mean_dist_prox_ratio, input_sequence)\n",
    "    if subtract_avg :\n",
    "        input_grad = (input_grad - tf.reduce_mean(input_grad, axis=-1, keepdims=True))\n",
    "    else :\n",
    "        input_grad = input_grad\n",
    "\n",
    "    return input_grad\n",
    "\n",
    "#Function to compute prediction gradients for list of sequences (with rc)\n",
    "def get_prediction_gradient_w_rc(models, sequence_one_hots, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, track_index, track_scale, track_transform, clip_soft=None, prox_bin_index=None, dist_bin_index=None, use_mean=False, use_ratio=True, use_logodds=False, subtract_avg=False, fold_index=[0, 1, 2, 3]) :\n",
    "    \n",
    "    #Get gradients for fwd\n",
    "    pred_grads = get_prediction_gradient(models, sequence_one_hots, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, track_index, track_scale, track_transform, clip_soft, prox_bin_index, dist_bin_index, use_mean, use_ratio, use_logodds, subtract_avg, fold_index)\n",
    "    \n",
    "    #Get sequences for rev\n",
    "    sequence_one_hots_rc = [\n",
    "        sequence_one_hots[example_ix][::-1, ::-1] for example_ix in range(len(sequence_one_hots))\n",
    "    ]\n",
    "    \n",
    "    #Invert start/end of distal and proximal bins\n",
    "    prox_bin_start_rc = models[0].target_lengths[0] - prox_bin_start - 1\n",
    "    prox_bin_end_rc = models[0].target_lengths[0] - prox_bin_end - 1\n",
    "    \n",
    "    dist_bin_start_rc = models[0].target_lengths[0] - dist_bin_start - 1\n",
    "    dist_bin_end_rc = models[0].target_lengths[0] - dist_bin_end - 1\n",
    "    \n",
    "    #Invert bin index vectors\n",
    "    prox_bin_index_rc = None\n",
    "    if prox_bin_index is not None :\n",
    "        prox_bin_index_rc = [models[0].target_lengths[0] - prox_bin - 1 for prox_bin in prox_bin_index]\n",
    "    \n",
    "    dist_bin_index_rc = None\n",
    "    if dist_bin_index is not None :\n",
    "        dist_bin_index_rc = [models[0].target_lengths[0] - dist_bin - 1 for dist_bin in dist_bin_index]\n",
    "    \n",
    "    #Get gradients for rev\n",
    "    pred_grads_rc = get_prediction_gradient(models, sequence_one_hots_rc, prox_bin_end_rc, prox_bin_start_rc, dist_bin_end_rc, dist_bin_start_rc, track_index, track_scale, track_transform, clip_soft, prox_bin_index_rc, dist_bin_index_rc, use_mean, use_ratio, use_logodds, subtract_avg, fold_index)\n",
    "    \n",
    "    pred_grads_avg = [\n",
    "        (pred_grads[example_ix] + pred_grads_rc[example_ix][::-1, ::-1]) / 2. for example_ix in range(len(sequence_one_hots))\n",
    "    ]\n",
    "    \n",
    "    return pred_grads, pred_grads_rc, pred_grads_avg\n",
    "\n",
    "#Function to compute prediction gradients for list of sequences\n",
    "def get_prediction_gradient(models, sequence_one_hots, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, track_index, track_scale, track_transform, clip_soft=None, prox_bin_index=None, dist_bin_index=None, use_mean=False, use_ratio=True, use_logodds=False, subtract_avg=False, fold_index=[0, 1, 2, 3]) :\n",
    "    \n",
    "    pred_grads = np.zeros((len(sequence_one_hots), len(fold_index), 524288, 4))\n",
    "    \n",
    "    #Loop over folds\n",
    "    for fold_i, fold_ix in enumerate(fold_index) :\n",
    "        \n",
    "        prediction_model = models[fold_ix].model.layers[1]\n",
    "        \n",
    "        #Create new input tensor\n",
    "        input_sequence = tf.keras.layers.Input(shape=(524288, 4), name='sequence')\n",
    "\n",
    "        #Wrap prediction gradient TF function in a lambda layer\n",
    "        input_grad = tf.keras.layers.Lambda(lambda x: _prediction_input_grad(x, prediction_model, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, track_index, track_scale, track_transform, clip_soft, use_mean, use_ratio, use_logodds, subtract_avg, prox_bin_index, dist_bin_index), name='inp_grad')(input_sequence)\n",
    "\n",
    "        #Create new keras model that returns the gradient of predictions\n",
    "        grad_model = tf.keras.models.Model(input_sequence, input_grad)\n",
    "        \n",
    "        #Compute gradient on CPU\n",
    "        with tf.device('/cpu:0') :\n",
    "            for example_ix in range(len(sequence_one_hots)) :\n",
    "                pred_grads[example_ix, fold_i, ...] = sequence_one_hots[example_ix] * grad_model.predict(x=[sequence_one_hots[example_ix][None, ...]], batch_size=1, verbose=True)[0, ...]\n",
    "        \n",
    "        #Run garbage collection before next fold\n",
    "        prediction_model = None\n",
    "        gc.collect()\n",
    "    \n",
    "    #Average and mean-subtract\n",
    "    pred_grads = np.mean(pred_grads, axis=1)\n",
    "    pred_grads = [\n",
    "        np.sum(pred_grads[example_ix, ...], axis=-1, keepdims=True) * sequence_one_hots[example_ix] for example_ix in range(len(sequence_one_hots))\n",
    "    ]\n",
    "    \n",
    "    return pred_grads\n",
    "\n",
    "#Function to calculate prediction statistic for a subset of positions\n",
    "def _prediction_ism_score(pred, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, use_mean, use_ratio, use_logodds, prox_bin_index, dist_bin_index) :\n",
    "\n",
    "    #Aggregate over positions and optionally compute ratios\n",
    "    if not use_mean :\n",
    "        #Get sum of distal coverage\n",
    "        if dist_bin_index is None :\n",
    "            mean_dist = np.sum(pred[:, dist_bin_start:dist_bin_end], axis=1)\n",
    "        else :\n",
    "            mean_dist = np.sum(pred[:, dist_bin_index], axis=1)\n",
    "        \n",
    "        #Get mean proximal coverage\n",
    "        if prox_bin_index is None :\n",
    "            mean_prox = np.sum(pred[:, prox_bin_start:prox_bin_end], axis=1)\n",
    "        else :\n",
    "            mean_prox = np.sum(pred[:, prox_bin_index], axis=1)\n",
    "    else:\n",
    "        #Get sum of distal coverage\n",
    "        if dist_bin_index is None :\n",
    "            mean_dist = np.mean(pred[:, dist_bin_start:dist_bin_end], axis=1)\n",
    "        else :\n",
    "            mean_dist = np.mean(pred[:, dist_bin_index], axis=1)\n",
    "        \n",
    "        #Get mean proximal coverage\n",
    "        if prox_bin_index is None :\n",
    "            mean_prox = np.mean(pred[:, prox_bin_start:prox_bin_end], axis=1)\n",
    "        else :\n",
    "            mean_prox = np.mean(pred[:, prox_bin_index], axis=1)\n",
    "    \n",
    "    if not use_ratio :\n",
    "        #Get log-sum of coverage\n",
    "        mean_dist_prox_ratio = np.log(mean_dist + 1e-6)\n",
    "    else :\n",
    "        #Get ratio (or log odds) of distal and proximal coverage samples\n",
    "        if not use_logodds :\n",
    "            mean_dist_prox_ratio = np.log(mean_dist / mean_prox + 1e-6)\n",
    "        else :\n",
    "            mean_dist_prox_ratio = np.log((mean_dist / mean_prox) / (1. - (mean_dist / mean_prox)) + 1e-6)\n",
    "\n",
    "    return mean_dist_prox_ratio\n",
    "\n",
    "#Function to compute ISM scores over a range of positions, for a list of sequences\n",
    "def get_ism(models, sequence_one_hots, ism_start, ism_end, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, track_index, track_scale, track_transform, clip_soft, prox_bin_index=None, dist_bin_index=None, use_mean=False, use_ratio=True, use_logodds=False) :\n",
    "    \n",
    "    pred_ism = np.zeros((len(sequence_one_hots), len(models), 524288, 4))\n",
    "    \n",
    "    bases = [0, 1, 2, 3]\n",
    "    \n",
    "    #Loop over sequences\n",
    "    for example_ix in range(len(sequence_one_hots)) :\n",
    "        \n",
    "        print(\"example_ix = \" + str(example_ix))\n",
    "        \n",
    "        sequence_one_hot_wt = sequence_one_hots[example_ix]\n",
    "        \n",
    "        #Get pred\n",
    "        y_wt = predict_tracks(models, sequence_one_hot_wt)[0, ...][..., track_index].astype('float32')\n",
    "        \n",
    "        #Undo scale\n",
    "        y_wt /= track_scale\n",
    "\n",
    "        #Undo soft_clip\n",
    "        if clip_soft is not None :\n",
    "            y_wt_unclipped = (y_wt - clip_soft)**2 + clip_soft\n",
    "            unclip_mask_wt = (y_wt > clip_soft)\n",
    "\n",
    "            y_wt[unclip_mask_wt] = y_wt_unclipped[unclip_mask_wt]\n",
    "\n",
    "        #Undo sqrt\n",
    "        y_wt = y_wt**(1. / track_transform)\n",
    "        \n",
    "        #Aggregate over tracks (average)\n",
    "        y_wt = np.mean(y_wt, axis=-1)\n",
    "        \n",
    "        #Compute scalar statistics for wt sequence\n",
    "        score_wt = _prediction_ism_score(y_wt, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, use_mean, use_ratio, use_logodds, prox_bin_index, dist_bin_index)\n",
    "        \n",
    "        #Loop over positions in range\n",
    "        for j in range(ism_start, ism_end) :\n",
    "            for b in bases :\n",
    "                #Mutate sequence\n",
    "                if sequence_one_hot_wt[j, b] != 1. : \n",
    "                    sequence_one_hot_mut = np.copy(sequence_one_hot_wt)\n",
    "                    sequence_one_hot_mut[j, :] = 0.\n",
    "                    sequence_one_hot_mut[j, b] = 1.\n",
    "                    \n",
    "                    #Get pred\n",
    "                    y_mut = predict_tracks(models, sequence_one_hot_mut)[0, ...][..., track_index].astype('float32')\n",
    "\n",
    "                    #Undo scale\n",
    "                    y_mut /= track_scale\n",
    "\n",
    "                    #Undo soft_clip\n",
    "                    if clip_soft is not None :\n",
    "                        y_mut_unclipped = (y_mut - clip_soft)**2 + clip_soft\n",
    "                        unclip_mask_mut = (y_mut > clip_soft)\n",
    "\n",
    "                        y_mut[unclip_mask_mut] = y_mut_unclipped[unclip_mask_mut]\n",
    "\n",
    "                    #Undo sqrt\n",
    "                    y_mut = y_mut**(1. / track_transform)\n",
    "\n",
    "                    #Aggregate over tracks (average)\n",
    "                    y_mut = np.mean(y_mut, axis=-1)\n",
    "                    \n",
    "                    #Compute scalar statistic for mutated sequence\n",
    "                    score_mut = _prediction_ism_score(y_mut, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, use_mean, use_ratio, use_logodds, prox_bin_index, dist_bin_index)\n",
    "                    \n",
    "                    pred_ism[example_ix, :, j, b] = score_wt - score_mut\n",
    "        \n",
    "        #Average across nucleotides\n",
    "        pred_ism[example_ix, ...] = np.tile(np.mean(pred_ism[example_ix, ...], axis=-1)[..., None], (1, 1, 4)) * sequence_one_hots[example_ix][None, ...]\n",
    "\n",
    "    #Average across folds\n",
    "    pred_ism = np.mean(pred_ism, axis=1)\n",
    "    pred_ism = [pred_ism[example_ix, ...] for example_ix in range(len(sequence_one_hots))]\n",
    "    \n",
    "    return pred_ism\n",
    "\n",
    "#Function to compute ISM shuffle scores over a range of positions, for a list of sequences\n",
    "def get_ism_shuffle(models, sequence_one_hots, ism_start, ism_end, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, track_index, track_scale, track_transform, clip_soft, prox_bin_index=None, dist_bin_index=None, window_size=5, n_samples=8, mononuc_shuffle=False, dinuc_shuffle=False, use_mean=False, use_ratio=True, use_logodds=False) :\n",
    "    \n",
    "    pred_shuffle = np.zeros((len(sequence_one_hots), len(models), 524288, n_samples))\n",
    "    pred_ism = np.zeros((len(sequence_one_hots), len(models), 524288, 4))\n",
    "    \n",
    "    bases = [0, 1, 2, 3]\n",
    "    \n",
    "    #Loop over sequences\n",
    "    for example_ix in range(len(sequence_one_hots)) :\n",
    "        \n",
    "        print(\"example_ix = \" + str(example_ix))\n",
    "        \n",
    "        sequence_one_hot_wt = sequence_one_hots[example_ix]\n",
    "    \n",
    "        #Get pred\n",
    "        y_wt = predict_tracks(models, sequence_one_hot_wt)[0, ...][..., track_index].astype('float32')\n",
    "        \n",
    "        #Undo scale\n",
    "        y_wt /= track_scale\n",
    "\n",
    "        #Undo soft_clip\n",
    "        if clip_soft is not None :\n",
    "            y_wt_unclipped = (y_wt - clip_soft)**2 + clip_soft\n",
    "            unclip_mask_wt = (y_wt > clip_soft)\n",
    "\n",
    "            y_wt[unclip_mask_wt] = y_wt_unclipped[unclip_mask_wt]\n",
    "\n",
    "        #Undo sqrt\n",
    "        y_wt = y_wt**(1. / track_transform)\n",
    "        \n",
    "        #Aggregate over tracks (average)\n",
    "        y_wt = np.mean(y_wt, axis=-1)\n",
    "        \n",
    "        #Compute scalar statistics for wt sequence\n",
    "        score_wt = _prediction_ism_score(y_wt, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, use_mean, use_ratio, use_logodds, prox_bin_index, dist_bin_index)\n",
    "        \n",
    "        #Loop over ISM position range\n",
    "        for j in range(ism_start, ism_end) :\n",
    "            \n",
    "            j_start = j - window_size // 2\n",
    "            j_end = j + window_size // 2 + 1\n",
    "            \n",
    "            pos_index = np.arange(j_end-j_start)+j_start\n",
    "            \n",
    "            #Loop over samples at position j\n",
    "            for sample_ix in range(n_samples) :\n",
    "                sequence_one_hot_mut = np.copy(sequence_one_hot_wt)\n",
    "                sequence_one_hot_mut[j_start:j_end, :] = 0.\n",
    "                \n",
    "                #Randomly mutate\n",
    "                if not mononuc_shuffle and not dinuc_shuffle :\n",
    "                    nt_index = np.random.choice(bases, size=(j_end-j_start,)).tolist()\n",
    "                    sequence_one_hot_mut[pos_index, nt_index] = 1.\n",
    "                elif mononuc_shuffle :\n",
    "                    #Shuffle nucleotides\n",
    "                    shuffled_pos_index = np.copy(pos_index)\n",
    "                    np.random.shuffle(shuffled_pos_index)\n",
    "                    \n",
    "                    sequence_one_hot_mut[shuffled_pos_index, :] = sequence_one_hot_wt[pos_index, :]\n",
    "                else : #Dinucleotide-shuffle\n",
    "                    if sample_ix % 2 == 0 :\n",
    "                        shuffled_pos_index = [\n",
    "                            [pos_index[pos_j], pos_index[pos_j+1]] if pos_j+1 < pos_index.shape[0] else [pos_index[pos_j]]\n",
    "                            for pos_j in range(0, pos_index.shape[0], 2)\n",
    "                        ]\n",
    "                    else : #Offset sequence by 1 to get other set of dinucleotides\n",
    "                        pos_index_rev = np.copy(pos_index)[::-1]\n",
    "                        shuffled_pos_index = [\n",
    "                            [pos_index_rev[pos_j], pos_index_rev[pos_j+1]] if pos_j+1 < pos_index_rev.shape[0] else [pos_index_rev[pos_j]]\n",
    "                            for pos_j in range(0, pos_index_rev.shape[0], 2)\n",
    "                        ]\n",
    "                    \n",
    "                    shuffled_shuffle_index = np.arange(len(shuffled_pos_index), dtype='int32')\n",
    "                    np.random.shuffle(shuffled_shuffle_index)\n",
    "                    \n",
    "                    #Shuffle position indices\n",
    "                    shuffled_pos_index_new = []\n",
    "                    for pos_tuple_i in range(len(shuffled_pos_index)) :\n",
    "                        shuffled_pos_index_new.extend(shuffled_pos_index[shuffled_shuffle_index[pos_tuple_i]])\n",
    "                    \n",
    "                    #Apply shuffling\n",
    "                    shuffled_pos_index = np.array(shuffled_pos_index_new, dtype='int32')\n",
    "                    sequence_one_hot_mut[shuffled_pos_index, :] = sequence_one_hot_wt[pos_index, :]\n",
    "\n",
    "                #Get pred\n",
    "                y_mut = predict_tracks(models, sequence_one_hot_mut)[0, ...][..., track_index].astype('float32')\n",
    "\n",
    "                #Undo scale\n",
    "                y_mut /= track_scale\n",
    "\n",
    "                #Undo soft_clip\n",
    "                if clip_soft is not None :\n",
    "                    y_mut_unclipped = (y_mut - clip_soft)**2 + clip_soft\n",
    "                    unclip_mask_mut = (y_mut > clip_soft)\n",
    "\n",
    "                    y_mut[unclip_mask_mut] = y_mut_unclipped[unclip_mask_mut]\n",
    "\n",
    "                #Undo sqrt\n",
    "                y_mut = y_mut**(1. / track_transform)\n",
    "\n",
    "                #Aggregate over tracks (average)\n",
    "                y_mut = np.mean(y_mut, axis=-1)\n",
    "                \n",
    "                #Compute scalar statistics for mutated sequence\n",
    "                score_mut = _prediction_ism_score(y_mut, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, use_mean, use_ratio, use_logodds, prox_bin_index, dist_bin_index)\n",
    "\n",
    "                pred_shuffle[example_ix, :, j, sample_ix] = score_wt - score_mut\n",
    "\n",
    "        pred_ism[example_ix, ...] = np.tile(np.mean(pred_shuffle[example_ix, ...], axis=-1)[..., None], (1, 1, 4)) * sequence_one_hots[example_ix][None, ...]\n",
    "\n",
    "    pred_ism = np.mean(pred_ism, axis=1)\n",
    "    pred_ism = [pred_ism[example_ix, ...] for example_ix in range(len(sequence_one_hots))]\n",
    "    \n",
    "    return pred_ism\n",
    "\n",
    "#Function to plot sequence logo\n",
    "def plot_seq_scores(importance_scores, figsize=(16, 2), plot_y_ticks=True, y_min=None, y_max=None, save_figs=False, fig_name=\"default\") :\n",
    "\n",
    "    importance_scores = importance_scores.T\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    ref_seq = \"\"\n",
    "    \n",
    "    #Loop over reference sequence letters\n",
    "    for j in range(importance_scores.shape[1]) :\n",
    "        argmax_nt = np.argmax(np.abs(importance_scores[:, j]))\n",
    "        \n",
    "        if argmax_nt == 0 :\n",
    "            ref_seq += \"A\"\n",
    "        elif argmax_nt == 1 :\n",
    "            ref_seq += \"C\"\n",
    "        elif argmax_nt == 2 :\n",
    "            ref_seq += \"G\"\n",
    "        elif argmax_nt == 3 :\n",
    "            ref_seq += \"T\"\n",
    "\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    #Loop over reference sequence letters and draw\n",
    "    for i in range(0, len(ref_seq)) :\n",
    "        mutability_score = np.sum(importance_scores[:, i])\n",
    "        color = None\n",
    "        dna_letter_at(ref_seq[i], i + 0.5, 0, mutability_score, ax, color=color)\n",
    "    \n",
    "    plt.sca(ax)\n",
    "    plt.xticks([], [])\n",
    "    plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "    \n",
    "    plt.xlim((0, len(ref_seq)))\n",
    "    \n",
    "    #plt.axis('off')\n",
    "    \n",
    "    if plot_y_ticks :\n",
    "        plt.yticks(fontsize=12)\n",
    "    else :\n",
    "        plt.yticks([], [])\n",
    "    \n",
    "    #Set axis limits\n",
    "    if y_min is not None and y_max is not None :\n",
    "        plt.ylim(y_min, y_max)\n",
    "    elif y_min is not None :\n",
    "        plt.ylim(y_min)\n",
    "    else :\n",
    "        plt.ylim(\n",
    "            np.min(importance_scores) - 0.1 * np.max(np.abs(importance_scores)),\n",
    "            np.max(importance_scores) + 0.1 * np.max(np.abs(importance_scores))\n",
    "        )\n",
    "    \n",
    "    plt.axhline(y=0., color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "    #for axis in fig.axes :\n",
    "    #    axis.get_xaxis().set_visible(False)\n",
    "    #    axis.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_figs :\n",
    "        plt.savefig(fig_name + \".png\", transparent=True, dpi=300)\n",
    "        plt.savefig(fig_name + \".eps\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#Plot pair of sequence logos with matched y-axis height\n",
    "def visualize_input_gradient_pair(att_grad_wt, att_grad_mut, plot_start=0, plot_end=100, save_figs=False, fig_name='') :\n",
    "\n",
    "    scores_wt = att_grad_wt[plot_start:plot_end, :]\n",
    "    scores_mut = att_grad_mut[plot_start:plot_end, :]\n",
    "\n",
    "    #Get logo bounds\n",
    "    y_min = min(np.min(scores_wt), np.min(scores_mut))\n",
    "    y_max = max(np.max(scores_wt), np.max(scores_mut))\n",
    "\n",
    "    y_max_abs = max(np.abs(y_min), np.abs(y_max))\n",
    "\n",
    "    y_min = y_min - 0.05 * y_max_abs\n",
    "    y_max = y_max + 0.05 * y_max_abs\n",
    "    \n",
    "    print(\"y_min = \" + str(round(y_min, 8)))\n",
    "    print(\"y_max = \" + str(round(y_max, 8)))\n",
    "\n",
    "    #Plot wt logo\n",
    "    print(\"--- WT ---\")\n",
    "    plot_seq_scores(\n",
    "        scores_wt, y_min=y_min, y_max=y_max,\n",
    "        figsize=(8, 1),\n",
    "        plot_y_ticks=False,\n",
    "        save_figs=save_figs,\n",
    "        fig_name=fig_name + '_wt',\n",
    "    )\n",
    "\n",
    "    #Plot mut logo\n",
    "    print(\"--- Mut ---\")\n",
    "    plot_seq_scores(\n",
    "        scores_mut, y_min=y_min, y_max=y_max,\n",
    "        figsize=(8, 1),\n",
    "        plot_y_ticks=False,\n",
    "        save_figs=save_figs,\n",
    "        fig_name=fig_name + '_mut',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize fasta sequence extractor\n",
    "import pyfaidx\n",
    "import pysam\n",
    "fasta_file = '/home/gl/projects/Borzoi/borzoi/examples/hg38/assembly/ucsc/hg38.fa'\n",
    "\n",
    "#!wget -O - http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz | gunzip -c > {fasta_file}\n",
    "pyfaidx.Faidx(fasta_file)\n",
    "\n",
    "fasta_open = pysam.Fastafile(fasta_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define default annotation settings\n",
    "\n",
    "annotate_features = [\n",
    "    {'feature' : 'gene', 'annotate_text' : True, 'filter_query' : \"Strand == '+'\", 'plot_type' : 'box', 'color' : 'lightgreen', 'marker' : None, 'min_len' : 4096, 'max_len' : 1e9},\n",
    "    {'feature' : 'gene', 'annotate_text' : True, 'filter_query' : \"Strand == '-'\", 'plot_type' : 'box', 'color' : 'deepskyblue', 'marker' : None, 'min_len' : 4096, 'max_len' : 1e9},\n",
    "    \n",
    "    {'feature' : 'five_prime_UTR', 'annotate_text' : False, 'filter_query' : None, 'plot_type' : 'line', 'color' : 'magenta', 'marker' : None, 'min_len' : 0, 'max_len' : 1e9},\n",
    "    {'feature' : 'three_prime_UTR', 'annotate_text' : False, 'filter_query' : None, 'plot_type' : 'line', 'color' : 'magenta', 'marker' : None, 'min_len' : 0, 'max_len' : 1e9},\n",
    "    \n",
    "    {'feature' : 'exon', 'annotate_text' : False, 'filter_query' : None, 'plot_type' : 'line', 'color' : 'deepskyblue', 'marker' : None, 'min_len' : 0, 'max_len' : 1e9},\n",
    "    \n",
    "    {'feature' : 'exon', 'annotate_text' : False, 'filter_query' : None, 'plot_type' : 'dots', 'color' : 'deepskyblue', 'marker' : \"*\", 'size' : 50, 'min_len' : 0, 'max_len' : 1e9},\n",
    "    {'feature' : 'polyA_site', 'annotate_text' : False, 'filter_query' : None, 'plot_type' : 'dots', 'color' : 'red', 'marker' : \"*\", 'size' : 50, 'min_len' : 0, 'max_len' : 1e9},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 13:21:56.440219: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'StochasticReverseComplement' object has no attribute 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m70517808\u001b[39m\n\u001b[1;32m      7\u001b[0m sequence_one_hot, annotation_df \u001b[38;5;241m=\u001b[39m process_sequence(chrom, start, end)\n\u001b[0;32m----> 9\u001b[0m predicted_tracks, attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_tracks_and_attention_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_one_hot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_rc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 146\u001b[0m, in \u001b[0;36mpredict_tracks_and_attention_scores\u001b[0;34m(models, sequence_one_hot, track_scale, track_transform, clip_soft, n_layers, score_rc)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m#Loop over layers\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_ix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layers) :\n\u001b[1;32m    144\u001b[0m \n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m#Get attention score model and make predictions\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m     attention_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_attention_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfold_ix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_ix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_ix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     att_scores \u001b[38;5;241m=\u001b[39m attention_model\u001b[38;5;241m.\u001b[39mpredict(x\u001b[38;5;241m=\u001b[39m[sequence_one_hot[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m#Optionally reverse-complement\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 93\u001b[0m, in \u001b[0;36mget_attention_model\u001b[0;34m(seqnn_model, layer_ix, inital_offset, offset)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_attention_model\u001b[39m(seqnn_model, layer_ix\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, inital_offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m) :\n\u001b[1;32m     90\u001b[0m \n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m#Create new model object. \u001b[39;00m\n\u001b[1;32m     92\u001b[0m     attention_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel(\n\u001b[0;32m---> 93\u001b[0m         \u001b[43mseqnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m, \u001b[38;5;66;03m# modified: inputs -> input\u001b[39;00m\n\u001b[1;32m     94\u001b[0m         _get_attention_weights(\n\u001b[1;32m     95\u001b[0m             seqnn_model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlayers[inital_offset \u001b[38;5;241m+\u001b[39m offset \u001b[38;5;241m*\u001b[39m layer_ix],\n\u001b[1;32m     96\u001b[0m             seqnn_model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlayers[inital_offset \u001b[38;5;241m+\u001b[39m offset \u001b[38;5;241m*\u001b[39m layer_ix \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39moutput,\n\u001b[1;32m     97\u001b[0m         ),\n\u001b[1;32m     98\u001b[0m     )\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attention_model\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StochasticReverseComplement' object has no attribute 'inputs'"
     ]
    }
   ],
   "source": [
    "#Predict for example sequence chr1:69993520-70517808\n",
    "\n",
    "chrom = 'chr1'\n",
    "start = 69993520\n",
    "end = 70517808\n",
    "\n",
    "sequence_one_hot, annotation_df = process_sequence(chrom, start, end)\n",
    "\n",
    "predicted_tracks, attention_scores = predict_tracks_and_attention_scores(models, sequence_one_hot, score_rc=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = f'/home/gl/projects/Borzoi/borzoi/examples/saved_models/f3c{fold_ix}/train/model0_best.h5'\n",
    "seqnn_model = seqnn.SeqNN(params_model)\n",
    "seqnn_model.restore(model_file, 0, trunk=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印模型结构\n",
    "print(\"Model structure:\")\n",
    "print(seqnn_model.model.summary())\n",
    "\n",
    "# 打印第二层的类型和属性\n",
    "layer = seqnn_model.model.layers[1]\n",
    "print(\"\\nLayer type:\", type(layer))\n",
    "print(\"Layer attributes:\", dir(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:\n",
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " sequence (InputLayer)       [(None, 524288, 4)]          0         []                            \n",
      "                                                                                                  \n",
      " stochastic_reverse_complem  ((None, 524288, 4),          0         ['sequence[0][0]']            \n",
      " ent_4 (StochasticReverseCo   ())                                                                 \n",
      " mplement)                                                                                        \n",
      "                                                                                                  \n",
      " stochastic_shift_4 (Stocha  (None, 524288, 4)            0         ['stochastic_reverse_complemen\n",
      " sticShift)                                                         t_4[0][0]']                   \n",
      "                                                                                                  \n",
      " conv1d_32 (Conv1D)          (None, 524288, 512)          31232     ['stochastic_shift_4[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling1d_28 (MaxPooli  (None, 262144, 512)          0         ['conv1d_32[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " batch_normalization_44 (Ba  (None, 262144, 512)          2048      ['max_pooling1d_28[0][0]']    \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.nn.gelu_48 (TFOpLambda)  (None, 262144, 512)          0         ['batch_normalization_44[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_33 (Conv1D)          (None, 262144, 608)          1557088   ['tf.nn.gelu_48[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d_29 (MaxPooli  (None, 131072, 608)          0         ['conv1d_33[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " batch_normalization_45 (Ba  (None, 131072, 608)          2432      ['max_pooling1d_29[0][0]']    \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.nn.gelu_49 (TFOpLambda)  (None, 131072, 608)          0         ['batch_normalization_45[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_34 (Conv1D)          (None, 131072, 736)          2238176   ['tf.nn.gelu_49[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d_30 (MaxPooli  (None, 65536, 736)           0         ['conv1d_34[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " batch_normalization_46 (Ba  (None, 65536, 736)           2944      ['max_pooling1d_30[0][0]']    \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.nn.gelu_50 (TFOpLambda)  (None, 65536, 736)           0         ['batch_normalization_46[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_35 (Conv1D)          (None, 65536, 896)           3298176   ['tf.nn.gelu_50[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d_31 (MaxPooli  (None, 32768, 896)           0         ['conv1d_35[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " batch_normalization_47 (Ba  (None, 32768, 896)           3584      ['max_pooling1d_31[0][0]']    \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.nn.gelu_51 (TFOpLambda)  (None, 32768, 896)           0         ['batch_normalization_47[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_36 (Conv1D)          (None, 32768, 1056)          4731936   ['tf.nn.gelu_51[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d_32 (MaxPooli  (None, 16384, 1056)          0         ['conv1d_36[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " batch_normalization_48 (Ba  (None, 16384, 1056)          4224      ['max_pooling1d_32[0][0]']    \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.nn.gelu_52 (TFOpLambda)  (None, 16384, 1056)          0         ['batch_normalization_48[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_37 (Conv1D)          (None, 16384, 1280)          6759680   ['tf.nn.gelu_52[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d_33 (MaxPooli  (None, 8192, 1280)           0         ['conv1d_37[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " batch_normalization_49 (Ba  (None, 8192, 1280)           5120      ['max_pooling1d_33[0][0]']    \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.nn.gelu_53 (TFOpLambda)  (None, 8192, 1280)           0         ['batch_normalization_49[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_38 (Conv1D)          (None, 8192, 1536)           9831936   ['tf.nn.gelu_53[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d_34 (MaxPooli  (None, 4096, 1536)           0         ['conv1d_38[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " layer_normalization_64 (La  (None, 4096, 1536)           3072      ['max_pooling1d_34[0][0]']    \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multihead_attention_32 (Mu  (None, 4096, 1536)           6310400   ['layer_normalization_64[0][0]\n",
      " ltiheadAttention)                                                  ']                            \n",
      "                                                                                                  \n",
      " dropout_100 (Dropout)       (None, 4096, 1536)           0         ['multihead_attention_32[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_72 (Add)                (None, 4096, 1536)           0         ['max_pooling1d_34[0][0]',    \n",
      "                                                                     'dropout_100[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_65 (La  (None, 4096, 1536)           3072      ['add_72[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_88 (Dense)            (None, 4096, 3072)           4721664   ['layer_normalization_65[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_101 (Dropout)       (None, 4096, 3072)           0         ['dense_88[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_33 (ReLU)             (None, 4096, 3072)           0         ['dropout_101[0][0]']         \n",
      "                                                                                                  \n",
      " dense_89 (Dense)            (None, 4096, 1536)           4720128   ['re_lu_33[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_102 (Dropout)       (None, 4096, 1536)           0         ['dense_89[0][0]']            \n",
      "                                                                                                  \n",
      " add_73 (Add)                (None, 4096, 1536)           0         ['add_72[0][0]',              \n",
      "                                                                     'dropout_102[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_66 (La  (None, 4096, 1536)           3072      ['add_73[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multihead_attention_33 (Mu  (None, 4096, 1536)           6310400   ['layer_normalization_66[0][0]\n",
      " ltiheadAttention)                                                  ']                            \n",
      "                                                                                                  \n",
      " dropout_103 (Dropout)       (None, 4096, 1536)           0         ['multihead_attention_33[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_74 (Add)                (None, 4096, 1536)           0         ['add_73[0][0]',              \n",
      "                                                                     'dropout_103[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_67 (La  (None, 4096, 1536)           3072      ['add_74[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_90 (Dense)            (None, 4096, 3072)           4721664   ['layer_normalization_67[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_104 (Dropout)       (None, 4096, 3072)           0         ['dense_90[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_34 (ReLU)             (None, 4096, 3072)           0         ['dropout_104[0][0]']         \n",
      "                                                                                                  \n",
      " dense_91 (Dense)            (None, 4096, 1536)           4720128   ['re_lu_34[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_105 (Dropout)       (None, 4096, 1536)           0         ['dense_91[0][0]']            \n",
      "                                                                                                  \n",
      " add_75 (Add)                (None, 4096, 1536)           0         ['add_74[0][0]',              \n",
      "                                                                     'dropout_105[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_68 (La  (None, 4096, 1536)           3072      ['add_75[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multihead_attention_34 (Mu  (None, 4096, 1536)           6310400   ['layer_normalization_68[0][0]\n",
      " ltiheadAttention)                                                  ']                            \n",
      "                                                                                                  \n",
      " dropout_106 (Dropout)       (None, 4096, 1536)           0         ['multihead_attention_34[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_76 (Add)                (None, 4096, 1536)           0         ['add_75[0][0]',              \n",
      "                                                                     'dropout_106[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_69 (La  (None, 4096, 1536)           3072      ['add_76[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_92 (Dense)            (None, 4096, 3072)           4721664   ['layer_normalization_69[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_107 (Dropout)       (None, 4096, 3072)           0         ['dense_92[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_35 (ReLU)             (None, 4096, 3072)           0         ['dropout_107[0][0]']         \n",
      "                                                                                                  \n",
      " dense_93 (Dense)            (None, 4096, 1536)           4720128   ['re_lu_35[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_108 (Dropout)       (None, 4096, 1536)           0         ['dense_93[0][0]']            \n",
      "                                                                                                  \n",
      " add_77 (Add)                (None, 4096, 1536)           0         ['add_76[0][0]',              \n",
      "                                                                     'dropout_108[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_70 (La  (None, 4096, 1536)           3072      ['add_77[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multihead_attention_35 (Mu  (None, 4096, 1536)           6310400   ['layer_normalization_70[0][0]\n",
      " ltiheadAttention)                                                  ']                            \n",
      "                                                                                                  \n",
      " dropout_109 (Dropout)       (None, 4096, 1536)           0         ['multihead_attention_35[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_78 (Add)                (None, 4096, 1536)           0         ['add_77[0][0]',              \n",
      "                                                                     'dropout_109[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_71 (La  (None, 4096, 1536)           3072      ['add_78[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_94 (Dense)            (None, 4096, 3072)           4721664   ['layer_normalization_71[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_110 (Dropout)       (None, 4096, 3072)           0         ['dense_94[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_36 (ReLU)             (None, 4096, 3072)           0         ['dropout_110[0][0]']         \n",
      "                                                                                                  \n",
      " dense_95 (Dense)            (None, 4096, 1536)           4720128   ['re_lu_36[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_111 (Dropout)       (None, 4096, 1536)           0         ['dense_95[0][0]']            \n",
      "                                                                                                  \n",
      " add_79 (Add)                (None, 4096, 1536)           0         ['add_78[0][0]',              \n",
      "                                                                     'dropout_111[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_72 (La  (None, 4096, 1536)           3072      ['add_79[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multihead_attention_36 (Mu  (None, 4096, 1536)           6310400   ['layer_normalization_72[0][0]\n",
      " ltiheadAttention)                                                  ']                            \n",
      "                                                                                                  \n",
      " dropout_112 (Dropout)       (None, 4096, 1536)           0         ['multihead_attention_36[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_80 (Add)                (None, 4096, 1536)           0         ['add_79[0][0]',              \n",
      "                                                                     'dropout_112[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_73 (La  (None, 4096, 1536)           3072      ['add_80[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_96 (Dense)            (None, 4096, 3072)           4721664   ['layer_normalization_73[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_113 (Dropout)       (None, 4096, 3072)           0         ['dense_96[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_37 (ReLU)             (None, 4096, 3072)           0         ['dropout_113[0][0]']         \n",
      "                                                                                                  \n",
      " dense_97 (Dense)            (None, 4096, 1536)           4720128   ['re_lu_37[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_114 (Dropout)       (None, 4096, 1536)           0         ['dense_97[0][0]']            \n",
      "                                                                                                  \n",
      " add_81 (Add)                (None, 4096, 1536)           0         ['add_80[0][0]',              \n",
      "                                                                     'dropout_114[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_74 (La  (None, 4096, 1536)           3072      ['add_81[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multihead_attention_37 (Mu  (None, 4096, 1536)           6310400   ['layer_normalization_74[0][0]\n",
      " ltiheadAttention)                                                  ']                            \n",
      "                                                                                                  \n",
      " dropout_115 (Dropout)       (None, 4096, 1536)           0         ['multihead_attention_37[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_82 (Add)                (None, 4096, 1536)           0         ['add_81[0][0]',              \n",
      "                                                                     'dropout_115[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_75 (La  (None, 4096, 1536)           3072      ['add_82[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_98 (Dense)            (None, 4096, 3072)           4721664   ['layer_normalization_75[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_116 (Dropout)       (None, 4096, 3072)           0         ['dense_98[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_38 (ReLU)             (None, 4096, 3072)           0         ['dropout_116[0][0]']         \n",
      "                                                                                                  \n",
      " dense_99 (Dense)            (None, 4096, 1536)           4720128   ['re_lu_38[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_117 (Dropout)       (None, 4096, 1536)           0         ['dense_99[0][0]']            \n",
      "                                                                                                  \n",
      " add_83 (Add)                (None, 4096, 1536)           0         ['add_82[0][0]',              \n",
      "                                                                     'dropout_117[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_76 (La  (None, 4096, 1536)           3072      ['add_83[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multihead_attention_38 (Mu  (None, 4096, 1536)           6310400   ['layer_normalization_76[0][0]\n",
      " ltiheadAttention)                                                  ']                            \n",
      "                                                                                                  \n",
      " dropout_118 (Dropout)       (None, 4096, 1536)           0         ['multihead_attention_38[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_84 (Add)                (None, 4096, 1536)           0         ['add_83[0][0]',              \n",
      "                                                                     'dropout_118[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_77 (La  (None, 4096, 1536)           3072      ['add_84[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_100 (Dense)           (None, 4096, 3072)           4721664   ['layer_normalization_77[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_119 (Dropout)       (None, 4096, 3072)           0         ['dense_100[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_39 (ReLU)             (None, 4096, 3072)           0         ['dropout_119[0][0]']         \n",
      "                                                                                                  \n",
      " dense_101 (Dense)           (None, 4096, 1536)           4720128   ['re_lu_39[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_120 (Dropout)       (None, 4096, 1536)           0         ['dense_101[0][0]']           \n",
      "                                                                                                  \n",
      " add_85 (Add)                (None, 4096, 1536)           0         ['add_84[0][0]',              \n",
      "                                                                     'dropout_120[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_78 (La  (None, 4096, 1536)           3072      ['add_85[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multihead_attention_39 (Mu  (None, 4096, 1536)           6310400   ['layer_normalization_78[0][0]\n",
      " ltiheadAttention)                                                  ']                            \n",
      "                                                                                                  \n",
      " dropout_121 (Dropout)       (None, 4096, 1536)           0         ['multihead_attention_39[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_86 (Add)                (None, 4096, 1536)           0         ['add_85[0][0]',              \n",
      "                                                                     'dropout_121[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_79 (La  (None, 4096, 1536)           3072      ['add_86[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_102 (Dense)           (None, 4096, 3072)           4721664   ['layer_normalization_79[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_122 (Dropout)       (None, 4096, 3072)           0         ['dense_102[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_40 (ReLU)             (None, 4096, 3072)           0         ['dropout_122[0][0]']         \n",
      "                                                                                                  \n",
      " dense_103 (Dense)           (None, 4096, 1536)           4720128   ['re_lu_40[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_123 (Dropout)       (None, 4096, 1536)           0         ['dense_103[0][0]']           \n",
      "                                                                                                  \n",
      " add_87 (Add)                (None, 4096, 1536)           0         ['add_86[0][0]',              \n",
      "                                                                     'dropout_123[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_50 (Ba  (None, 4096, 1536)           6144      ['add_87[0][0]']              \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.nn.gelu_54 (TFOpLambda)  (None, 4096, 1536)           0         ['batch_normalization_50[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_51 (Ba  (None, 8192, 1536)           6144      ['conv1d_38[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_104 (Dense)           (None, 4096, 1536)           2360832   ['tf.nn.gelu_54[0][0]']       \n",
      "                                                                                                  \n",
      " tf.nn.gelu_55 (TFOpLambda)  (None, 8192, 1536)           0         ['batch_normalization_51[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " up_sampling1d_8 (UpSamplin  (None, 8192, 1536)           0         ['dense_104[0][0]']           \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " dense_105 (Dense)           (None, 8192, 1536)           2360832   ['tf.nn.gelu_55[0][0]']       \n",
      "                                                                                                  \n",
      " add_88 (Add)                (None, 8192, 1536)           0         ['up_sampling1d_8[0][0]',     \n",
      "                                                                     'dense_105[0][0]']           \n",
      "                                                                                                  \n",
      " separable_conv1d_8 (Separa  (None, 8192, 1536)           2365440   ['add_88[0][0]']              \n",
      " bleConv1D)                                                                                       \n",
      "                                                                                                  \n",
      " batch_normalization_52 (Ba  (None, 8192, 1536)           6144      ['separable_conv1d_8[0][0]']  \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.nn.gelu_56 (TFOpLambda)  (None, 8192, 1536)           0         ['batch_normalization_52[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_53 (Ba  (None, 16384, 1280)          5120      ['conv1d_37[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_106 (Dense)           (None, 8192, 1536)           2360832   ['tf.nn.gelu_56[0][0]']       \n",
      "                                                                                                  \n",
      " tf.nn.gelu_57 (TFOpLambda)  (None, 16384, 1280)          0         ['batch_normalization_53[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " up_sampling1d_9 (UpSamplin  (None, 16384, 1536)          0         ['dense_106[0][0]']           \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " dense_107 (Dense)           (None, 16384, 1536)          1967616   ['tf.nn.gelu_57[0][0]']       \n",
      "                                                                                                  \n",
      " add_89 (Add)                (None, 16384, 1536)          0         ['up_sampling1d_9[0][0]',     \n",
      "                                                                     'dense_107[0][0]']           \n",
      "                                                                                                  \n",
      " separable_conv1d_9 (Separa  (None, 16384, 1536)          2365440   ['add_89[0][0]']              \n",
      " bleConv1D)                                                                                       \n",
      "                                                                                                  \n",
      " cropping1d_4 (Cropping1D)   (None, 16352, 1536)          0         ['separable_conv1d_9[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_54 (Ba  (None, 16352, 1536)          6144      ['cropping1d_4[0][0]']        \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.nn.gelu_58 (TFOpLambda)  (None, 16352, 1536)          0         ['batch_normalization_54[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_39 (Conv1D)          (None, 16352, 1920)          2951040   ['tf.nn.gelu_58[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_124 (Dropout)       (None, 16352, 1920)          0         ['conv1d_39[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_59 (TFOpLambda)  (None, 16352, 1920)          0         ['dropout_124[0][0]']         \n",
      "                                                                                                  \n",
      " dense_108 (Dense)           (None, 16352, 7611)          1462073   ['tf.nn.gelu_59[0][0]']       \n",
      "                                                          1                                       \n",
      "                                                                                                  \n",
      " switch_reverse_16 (SwitchR  (None, 16352, 7611)          0         ['dense_108[0][0]',           \n",
      " everse)                                                             'stochastic_reverse_complemen\n",
      "                                                                    t_4[0][1]']                   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185917723 (709.22 MB)\n",
      "Trainable params: 185892699 (709.12 MB)\n",
      "Non-trainable params: 25024 (97.75 KB)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Layer 0: <class 'keras.src.engine.input_layer.InputLayer'>\n",
      "Layer 1: <class 'baskerville.layers.StochasticReverseComplement'>\n",
      "Layer 2: <class 'baskerville.layers.StochasticShift'>\n",
      "Layer 3: <class 'keras.src.layers.convolutional.conv1d.Conv1D'>\n",
      "Layer 4: <class 'keras.src.layers.pooling.max_pooling1d.MaxPooling1D'>\n",
      "Layer 5: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer 6: <class 'keras.src.layers.core.tf_op_layer.TFOpLambda'>\n",
      "Layer 7: <class 'keras.src.layers.convolutional.conv1d.Conv1D'>\n",
      "Layer 8: <class 'keras.src.layers.pooling.max_pooling1d.MaxPooling1D'>\n",
      "Layer 9: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer 10: <class 'keras.src.layers.core.tf_op_layer.TFOpLambda'>\n",
      "Layer 11: <class 'keras.src.layers.convolutional.conv1d.Conv1D'>\n",
      "Layer 12: <class 'keras.src.layers.pooling.max_pooling1d.MaxPooling1D'>\n",
      "Layer 13: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer 14: <class 'keras.src.layers.core.tf_op_layer.TFOpLambda'>\n",
      "Layer 15: <class 'keras.src.layers.convolutional.conv1d.Conv1D'>\n",
      "Layer 16: <class 'keras.src.layers.pooling.max_pooling1d.MaxPooling1D'>\n",
      "Layer 17: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer 18: <class 'keras.src.layers.core.tf_op_layer.TFOpLambda'>\n",
      "Layer 19: <class 'keras.src.layers.convolutional.conv1d.Conv1D'>\n",
      "Layer 20: <class 'keras.src.layers.pooling.max_pooling1d.MaxPooling1D'>\n",
      "Layer 21: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer 22: <class 'keras.src.layers.core.tf_op_layer.TFOpLambda'>\n",
      "Layer 23: <class 'keras.src.layers.convolutional.conv1d.Conv1D'>\n",
      "Layer 24: <class 'keras.src.layers.pooling.max_pooling1d.MaxPooling1D'>\n",
      "Layer 25: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer 26: <class 'keras.src.layers.core.tf_op_layer.TFOpLambda'>\n",
      "Layer 27: <class 'keras.src.layers.convolutional.conv1d.Conv1D'>\n",
      "Layer 28: <class 'keras.src.layers.pooling.max_pooling1d.MaxPooling1D'>\n",
      "Layer 29: <class 'keras.src.layers.normalization.layer_normalization.LayerNormalization'>\n",
      "Layer 30: <class 'baskerville.layers.MultiheadAttention'>\n",
      "Layer 31: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 32: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer 33: <class 'keras.src.layers.normalization.layer_normalization.LayerNormalization'>\n",
      "Layer 34: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 35: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 36: <class 'keras.src.layers.activation.relu.ReLU'>\n",
      "Layer 37: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 38: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 39: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer 40: <class 'keras.src.layers.normalization.layer_normalization.LayerNormalization'>\n",
      "Layer 41: <class 'baskerville.layers.MultiheadAttention'>\n",
      "Layer 42: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 43: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer 44: <class 'keras.src.layers.normalization.layer_normalization.LayerNormalization'>\n",
      "Layer 45: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 46: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 47: <class 'keras.src.layers.activation.relu.ReLU'>\n",
      "Layer 48: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 49: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 50: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer 51: <class 'keras.src.layers.normalization.layer_normalization.LayerNormalization'>\n",
      "Layer 52: <class 'baskerville.layers.MultiheadAttention'>\n",
      "Layer 53: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 54: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer 55: <class 'keras.src.layers.normalization.layer_normalization.LayerNormalization'>\n",
      "Layer 56: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 57: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 58: <class 'keras.src.layers.activation.relu.ReLU'>\n",
      "Layer 59: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 60: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 61: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer 62: <class 'keras.src.layers.normalization.layer_normalization.LayerNormalization'>\n",
      "Layer 63: <class 'baskerville.layers.MultiheadAttention'>\n",
      "Layer 64: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 65: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer 66: <class 'keras.src.layers.normalization.layer_normalization.LayerNormalization'>\n",
      "Layer 67: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 68: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 69: <class 'keras.src.layers.activation.relu.ReLU'>\n",
      "Layer 70: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 71: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 72: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer 73: <class 'keras.src.layers.normalization.layer_normalization.LayerNormalization'>\n",
      "Layer 74: <class 'baskerville.layers.MultiheadAttention'>\n",
      "Layer 75: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 76: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer 77: <class 'keras.src.layers.normalization.layer_normalization.LayerNormalization'>\n",
      "Layer 78: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 79: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 80: <class 'keras.src.layers.activation.relu.ReLU'>\n",
      "Layer 81: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 82: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 83: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer 84: <class 'keras.src.layers.normalization.layer_normalization.LayerNormalization'>\n",
      "Layer 85: <class 'baskerville.layers.MultiheadAttention'>\n",
      "Layer 86: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 87: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer 88: <class 'keras.src.layers.normalization.layer_normalization.LayerNormalization'>\n",
      "Layer 89: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 90: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 91: <class 'keras.src.layers.activation.relu.ReLU'>\n",
      "Layer 92: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 93: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 94: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer 95: <class 'keras.src.layers.normalization.layer_normalization.LayerNormalization'>\n",
      "Layer 96: <class 'baskerville.layers.MultiheadAttention'>\n",
      "Layer 97: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 98: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer 99: <class 'keras.src.layers.normalization.layer_normalization.LayerNormalization'>\n",
      "Layer 100: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 101: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 102: <class 'keras.src.layers.activation.relu.ReLU'>\n",
      "Layer 103: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 104: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 105: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer 106: <class 'keras.src.layers.normalization.layer_normalization.LayerNormalization'>\n",
      "Layer 107: <class 'baskerville.layers.MultiheadAttention'>\n",
      "Layer 108: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 109: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer 110: <class 'keras.src.layers.normalization.layer_normalization.LayerNormalization'>\n",
      "Layer 111: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 112: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 113: <class 'keras.src.layers.activation.relu.ReLU'>\n",
      "Layer 114: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 115: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 116: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer 117: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer 118: <class 'keras.src.layers.core.tf_op_layer.TFOpLambda'>\n",
      "Layer 119: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer 120: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 121: <class 'keras.src.layers.core.tf_op_layer.TFOpLambda'>\n",
      "Layer 122: <class 'keras.src.layers.reshaping.up_sampling1d.UpSampling1D'>\n",
      "Layer 123: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 124: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer 125: <class 'keras.src.layers.convolutional.separable_conv1d.SeparableConv1D'>\n",
      "Layer 126: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer 127: <class 'keras.src.layers.core.tf_op_layer.TFOpLambda'>\n",
      "Layer 128: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer 129: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 130: <class 'keras.src.layers.core.tf_op_layer.TFOpLambda'>\n",
      "Layer 131: <class 'keras.src.layers.reshaping.up_sampling1d.UpSampling1D'>\n",
      "Layer 132: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 133: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer 134: <class 'keras.src.layers.convolutional.separable_conv1d.SeparableConv1D'>\n",
      "Layer 135: <class 'keras.src.layers.reshaping.cropping1d.Cropping1D'>\n",
      "Layer 136: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer 137: <class 'keras.src.layers.core.tf_op_layer.TFOpLambda'>\n",
      "Layer 138: <class 'keras.src.layers.convolutional.conv1d.Conv1D'>\n",
      "Layer 139: <class 'keras.src.layers.regularization.dropout.Dropout'>\n",
      "Layer 140: <class 'keras.src.layers.core.tf_op_layer.TFOpLambda'>\n",
      "Layer 141: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer 142: <class 'baskerville.layers.SwitchReverse'>\n"
     ]
    }
   ],
   "source": [
    "# 首先打印模型结构\n",
    "print(\"Model structure:\")\n",
    "print(seqnn_model.model.summary())\n",
    "\n",
    "# 打印每一层的类型\n",
    "for i, layer in enumerate(seqnn_model.model.layers):\n",
    "    print(f\"Layer {i}: {type(layer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer type: <class 'baskerville.layers.StochasticReverseComplement'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "borzoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
