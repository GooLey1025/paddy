{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 16:41:10.796442: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-06 16:41:10.797719: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-06 16:41:10.815747: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-06 16:41:10.815764: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-06 16:41:10.816562: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-06 16:41:10.819998: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-06 16:41:10.820511: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-06 16:41:11.168390: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-05-06 16:41:11.416583: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-06 16:41:11.416739: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import baskerville.seqnn as seqnn\n",
    "\n",
    "targets_file = '/home/gl/projects/Borzoi/borzoi/examples/targets_gtex.txt'\n",
    "targets_df = pd.read_csv(targets_file, index_col=0, sep='\\t')\n",
    "target_index = targets_df.index\n",
    "\n",
    "\n",
    "params_file = \"/home/gl/projects/Borzoi/borzoi/examples/params_pred.json\"\n",
    "\n",
    "with open(params_file) as params_open :\n",
    "    params = json.load(params_open)\n",
    "    \n",
    "    params_model = params['model']\n",
    "    params_train = params['train']\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 524288\n",
    "model_file = '/home/gl/projects/Borzoi/borzoi/examples/saved_models/f3c3/train/model0_best.h5'\n",
    "seqnn_model = seqnn.SeqNN(params_model)\n",
    "seqnn_model.restore(model_file, 0, trunk=False)\n",
    "seqnn_model.build_ensemble(True, [0])\n",
    "models.append(seqnn_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(gene_pr) = 1077833\n",
      "len(polya_pr) = 92482\n",
      "[Final] len(gene_pr) = 1170315\n"
     ]
    }
   ],
   "source": [
    "import pyranges as pr\n",
    "\n",
    "# load gff file\n",
    "#exons_gff_file = '/home/gl/projects/Borzoi/borzoi/examples/hg38/genes/gencode41/gencode41_basic_nort_protein_exons.gff'\n",
    "polya_gff_file = '/home/gl/projects/Borzoi/borzoi/examples/hg38/genes/gencode41/gencode.v41.polyAs.gff3'\n",
    "gene_gff_file = '/home/gl/projects/Borzoi/borzoi/examples/hg38/genes/gencode41/gencode.v41.basic.annotation.gff3'\n",
    "\n",
    "\n",
    "#exons_pr = pr.read_gff3(exons_gff_file)\n",
    "#exons_pr = exons_pr[exons_pr.Feature == 'exon']\n",
    "#exons_pr = exons_pr.drop('Source')\n",
    "#exons_pr = exons_pr.drop('Score')\n",
    "\n",
    "#print(\"len(exons_pr) = \" + str(len(exons_pr)))\n",
    "\n",
    "gene_pr = pr.read_gff3(gene_gff_file)\n",
    "\n",
    "gene_pr = gene_pr[gene_pr.Feature.isin(['gene', 'exon', 'five_prime_UTR', 'three_prime_UTR'])]\n",
    "\n",
    "gene_pr = gene_pr.drop('Source')\n",
    "gene_pr = gene_pr.drop('Score')\n",
    "gene_pr = gene_pr.drop('Frame')\n",
    "gene_pr = gene_pr.drop('ID')\n",
    "gene_pr = gene_pr.drop('hgnc_id')\n",
    "gene_pr = gene_pr.drop('havana_gene')\n",
    "gene_pr = gene_pr.drop('Parent')\n",
    "gene_pr = gene_pr.drop('transcript_support_level')\n",
    "gene_pr = gene_pr.drop('tag')\n",
    "gene_pr = gene_pr.drop('havana_transcript')\n",
    "gene_pr = gene_pr.drop('ont')\n",
    "gene_pr = gene_pr.drop('protein_id')\n",
    "gene_pr = gene_pr.drop('ccdsid')\n",
    "gene_pr = gene_pr.drop('artif_dupl')\n",
    "gene_pr = gene_pr.drop('level')\n",
    "\n",
    "print(\"len(gene_pr) = \" + str(len(gene_pr)))\n",
    "\n",
    "\n",
    "polya_pr = pr.read_gff3(polya_gff_file)\n",
    "\n",
    "polya_pr = polya_pr[polya_pr.Feature.isin(['polyA_signal', 'polyA_site'])]\n",
    "\n",
    "polya_pr = polya_pr.drop('Source')\n",
    "polya_pr = polya_pr.drop('Score')\n",
    "polya_pr = polya_pr.drop('Frame')\n",
    "polya_pr = polya_pr.drop('ID')\n",
    "polya_pr = polya_pr.drop('gene_type')\n",
    "polya_pr = polya_pr.drop('gene_name')\n",
    "polya_pr = polya_pr.drop('transcript_type')\n",
    "polya_pr = polya_pr.drop('transcript_name')\n",
    "polya_pr = polya_pr.drop('level')\n",
    "\n",
    "print(\"len(polya_pr) = \" + str(len(polya_pr)))\n",
    "\n",
    "#Concatenate annotations\n",
    "gene_pr = pr.concat([gene_pr, polya_pr])\n",
    "\n",
    "print(\"[Final] len(gene_pr) = \" + str(len(gene_pr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib.text import TextPath\n",
    "from matplotlib.patches import PathPatch, Rectangle\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "import gc\n",
    "from baskerville import dna\n",
    "\n",
    "#Function to get a one-hot coded sequence pattern\n",
    "def make_seq_1hot(genome_open, chrm, start, end, seq_len):\n",
    "    if start < 0:\n",
    "        seq_dna = 'N'*(-start) + genome_open.fetch(chrm, 0, end)\n",
    "    else:\n",
    "        seq_dna = genome_open.fetch(chrm, start, end)\n",
    "    \n",
    "    #Extend to full length (pad)\n",
    "    if len(seq_dna) < seq_len:\n",
    "        seq_dna += 'N'*(seq_len-len(seq_dna))\n",
    "    \n",
    "    seq_1hot = dna.dna_1hot(seq_dna)\n",
    "    \n",
    "    return seq_1hot\n",
    "\n",
    "#Function to inject into model definition to get attention scores\n",
    "def _get_attention_weights(self, inputs, training=False) :\n",
    "    \n",
    "    #Initialise the projection layers\n",
    "    embedding_size = self._value_size * self._num_heads\n",
    "    seq_len = inputs.shape[1]\n",
    "\n",
    "    #Compute q, k and v as multi-headed projections of the inputs\n",
    "    q = self._multihead_output(self._q_layer, inputs)  # [B, H, T, K]\n",
    "    k = self._multihead_output(self._k_layer, inputs)  # [B, H, T, K]\n",
    "    v = self._multihead_output(self._v_layer, inputs)  # [B, H, T, V]\n",
    "\n",
    "    #Scale the query by the square-root of key size\n",
    "    if self._scaling :\n",
    "        q *= self._key_size**-0.5\n",
    "\n",
    "    #[B, H, T', T]\n",
    "    content_logits = tf.matmul(q + self._r_w_bias, k, transpose_b=True)\n",
    "\n",
    "    if self._num_position_features == 0 :\n",
    "        logits = content_logits\n",
    "    else :\n",
    "        #Project positions to form relative keys.\n",
    "        distances = tf.range(-seq_len + 1, seq_len, dtype=tf.float32)[tf.newaxis]\n",
    "        positional_encodings = basenji.layers.positional_features(\n",
    "              positions=distances,\n",
    "              feature_size=self._num_position_features,\n",
    "              seq_length=seq_len,\n",
    "              symmetric=self._relative_position_symmetric)\n",
    "        #[1, 2T-1, Cr]\n",
    "      \n",
    "        if training :\n",
    "            positional_encodings = tf.nn.dropout(\n",
    "                positional_encodings, rate=self._positional_dropout_rate)\n",
    "\n",
    "        #[1, H, 2T-1, K]\n",
    "        r_k = self._multihead_output(self._r_k_layer, positional_encodings)\n",
    "\n",
    "        #Add shifted relative logits to content logits.\n",
    "        if self._content_position_bias :\n",
    "            #[B, H, T', 2T-1]\n",
    "            relative_logits = tf.matmul(q + self._r_r_bias, r_k, transpose_b=True)\n",
    "        else :\n",
    "            #[1, H, 1, 2T-1]\n",
    "            relative_logits = tf.matmul(self._r_r_bias, r_k, transpose_b=True)\n",
    "            #[1, H, T', 2T-1]\n",
    "            relative_logits = tf.broadcast_to(relative_logits, shape=(1, self._num_heads, seq_len, 2*seq_len-1))\n",
    "\n",
    "        #[B, H, T', T]\n",
    "        relative_logits = basenji.layers.relative_shift(relative_logits)\n",
    "        logits = content_logits + relative_logits\n",
    "\n",
    "    #Apply Softmax across length\n",
    "    weights = tf.nn.softmax(logits)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "#Function to create a new Keras model, outputting a specific attention layer's scores\n",
    "def get_attention_model(seqnn_model, layer_ix=0, inital_offset=30, offset=11) :\n",
    "\n",
    "    #Create new model object\n",
    "    attention_model = tf.keras.Model(\n",
    "        inputs=seqnn_model.model.layers[1].inputs,\n",
    "        outputs=_get_attention_weights(\n",
    "            seqnn_model.model.layers[1].layers[inital_offset + offset * layer_ix],\n",
    "            seqnn_model.model.layers[1].layers[inital_offset + offset * layer_ix - 1].output,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return attention_model\n",
    "\n",
    "#Function to predict tracks and attention scores\n",
    "def predict_tracks_and_attention_scores(models, sequence_one_hot, track_scale=1., track_transform=1., clip_soft=None, n_layers=8, score_rc=True) :\n",
    "    \n",
    "    attention_scores = []\n",
    "    predicted_tracks = []\n",
    "    \n",
    "    #Loop over folds\n",
    "    for fold_ix in range(n_folds) :\n",
    "\n",
    "        attention_scores_for_fold = []\n",
    "\n",
    "        yh = models[fold_ix](sequence_one_hot[None, ...])[:, None, ...].astype('float32')\n",
    "        \n",
    "        #Undo scale\n",
    "        yh /= track_scale\n",
    "\n",
    "        #Undo soft_clip\n",
    "        if clip_soft is not None :\n",
    "            yh_unclipped = (yh - clip_soft)**2 + clip_soft\n",
    "            unclip_mask_h = (yh > clip_soft)\n",
    "\n",
    "            yh[unclip_mask_h] = yh_unclipped[unclip_mask_h]\n",
    "\n",
    "        #Undo sqrt\n",
    "        yh = yh**(1. / track_transform)\n",
    "\n",
    "        #Aggregate over tracks (average)\n",
    "        ##yh = np.mean(yh, axis=-1)\n",
    "\n",
    "        predicted_tracks.append(yh)\n",
    "\n",
    "        #Loop over layers\n",
    "        for layer_ix in range(n_layers) :\n",
    "\n",
    "            #Get attention score model and make predictions\n",
    "            attention_model = get_attention_model(models[fold_ix], layer_ix=layer_ix)\n",
    "            att_scores = attention_model.predict(x=[sequence_one_hot[None, ...]], batch_size=1)\n",
    "            \n",
    "            #Optionally reverse-complement\n",
    "            if score_rc :\n",
    "                att_scores_rc = attention_model.predict(x=[sequence_one_hot[None, ::-1, ::-1]], batch_size=1)\n",
    "                att_scores = (att_scores + att_scores_rc[..., ::-1, ::-1]) / 2.\n",
    "\n",
    "            attention_scores_for_fold.append(att_scores[:, None, None, ...])\n",
    "\n",
    "            attention_model = None\n",
    "            gc.collect()\n",
    "\n",
    "        #Concatenate\n",
    "        attention_scores_for_fold = np.concatenate(attention_scores_for_fold, axis=2)\n",
    "        attention_scores.append(attention_scores_for_fold)\n",
    "\n",
    "    #Concatenate\n",
    "    predicted_tracks = np.concatenate(predicted_tracks, axis=1)\n",
    "    attention_scores = np.concatenate(attention_scores, axis=1)\n",
    "\n",
    "    print(\"predicted_tracks.shape = \" + str(predicted_tracks.shape))\n",
    "    print(\"attention_scores.shape = \" + str(attention_scores.shape))\n",
    "    \n",
    "    return predicted_tracks, attention_scores\n",
    "\n",
    "#Function to predict tracks\n",
    "def predict_tracks(models, sequence_one_hot, track_scale=1., track_transform=1., clip_soft=None) :\n",
    "    \n",
    "    predicted_tracks = []\n",
    "    \n",
    "    #Loop over folds\n",
    "    for fold_ix in range(n_folds) :\n",
    "\n",
    "        yh = models[fold_ix](sequence_one_hot[None, ...])[:, None, ...].astype('float32')\n",
    "        \n",
    "        #Undo scale\n",
    "        yh /= track_scale\n",
    "\n",
    "        #Undo soft_clip\n",
    "        if clip_soft is not None :\n",
    "            yh_unclipped = (yh - clip_soft)**2 + clip_soft\n",
    "            unclip_mask_h = (yh > clip_soft)\n",
    "\n",
    "            yh[unclip_mask_h] = yh_unclipped[unclip_mask_h]\n",
    "\n",
    "        #Undo sqrt\n",
    "        yh = yh**(1. / track_transform)\n",
    "\n",
    "        predicted_tracks.append(yh)\n",
    "\n",
    "    #Concatenate\n",
    "    predicted_tracks = np.concatenate(predicted_tracks, axis=1)\n",
    "    print(\"predicted_tracks.shape = \" + str(predicted_tracks.shape))\n",
    "    \n",
    "    return predicted_tracks\n",
    "\n",
    "#Helper function to get (padded) one-hot and annotated sub-dataframe\n",
    "def process_sequence(chrom, start, end) :\n",
    "    \n",
    "    seq_len = end - start\n",
    "\n",
    "    #Pad sequence to input window size\n",
    "    start -= (SEQUENCE_LENGTH - seq_len) // 2\n",
    "    end += (SEQUENCE_LENGTH - seq_len) // 2\n",
    "\n",
    "    annotation_df = gene_pr.df.query(\"Chromosome == '\" + chrom + \"' and ((End >= \" + str(int(start)) + \" and End < \" + str(int(end)) + \") or (Start >= \" + str(int(start)) + \" and Start < \" + str(int(end)) + \"))\")\n",
    "\n",
    "    #Get one-hot\n",
    "    sequence_one_hot = make_seq_1hot(fasta_open, chrom, start, end, seq_len)\n",
    "    \n",
    "    return sequence_one_hot.astype('float32'), annotation_df\n",
    "\n",
    "#Function for visualizing attention scores and predicted tracks\n",
    "def plot_attention_score(predicted_tracks, attention_scores, chrom='chr1', start=0, end=1024, track_crop=16, track_pool=1, use_gaussian=False, gaussian_sigma=8, gaussian_truncate=2, plot_start=0, plot_end=1024, save_suffix='', vmin=0.0001, vmax=0.005, highlight_area=False, annotate_features=[], highlight_start=0, highlight_end=1, highlight_start_y=None, highlight_end_y=None, example_ix=0, track_scale_qtl=0.95, track_scale_val=None, track_scale=0.02, track_clip=0.08, fold_index=[0, 1, 2, 3], layer_index=[5, 6], head_index=[0, 1, 2, 3, 4, 5, 6, 7], figsize=(8, 8), fig_dpi=600, save_figs=False) :\n",
    "\n",
    "    #Average over tracks\n",
    "    yh = np.mean(predicted_tracks, axis=1)[example_ix, ...]\n",
    "    \n",
    "    #Pool track bins\n",
    "    if track_pool > 1 :\n",
    "        yh = np.mean(np.reshape(yh, (yh.shape[0] // track_pool, track_pool)), axis=-1)\n",
    "    \n",
    "    track_crop_pooled = track_crop // 4\n",
    "    \n",
    "    #Calculate track-to-attention ratio\n",
    "    track_ratio = yh.shape[0] // (4096 - 2 * track_crop_pooled)\n",
    "    \n",
    "    if track_scale_val is None :\n",
    "        track_scale_val = round(np.quantile(yh, q=track_scale_qtl), 4)\n",
    "        print(\"track_scale_val = \" + str(track_scale_val))\n",
    "    \n",
    "    #Zero-pad\n",
    "    yh = np.concatenate([np.zeros(track_crop_pooled * track_ratio), yh / track_scale_val, np.zeros(track_crop_pooled * track_ratio)], axis=0)\n",
    "    \n",
    "    #Average attention map\n",
    "    att = np.mean(\n",
    "        np.mean(\n",
    "            np.mean(\n",
    "                attention_scores[:, fold_index, ...]\n",
    "                , axis=1\n",
    "            )[:, layer_index, ...]\n",
    "            , axis=1\n",
    "        )[:, head_index, ...]\n",
    "        , axis=1\n",
    "    )[example_ix, ...]\n",
    "    \n",
    "    \n",
    "    #Optionally apply gaussian smoothing to attention map\n",
    "    if use_gaussian :\n",
    "        att = gaussian_filter(att, sigma=gaussian_sigma, truncate=gaussian_truncate)\n",
    "\n",
    "    plot_start_bin = (plot_start - start) // 128\n",
    "    plot_end_bin = (plot_end - start) // 128\n",
    "\n",
    "    track_scale = int(track_scale * (plot_end_bin - plot_start_bin))\n",
    "    track_clip = int(track_clip * (plot_end_bin - plot_start_bin))\n",
    "\n",
    "    highlight_start_x_bin = (highlight_start - start) // 128\n",
    "    highlight_end_x_bin = (highlight_end - start) // 128\n",
    "    \n",
    "    if highlight_start_y is not None and highlight_end_y is not None :\n",
    "        highlight_start_y_bin = (highlight_start_y - start) // 128\n",
    "        highlight_end_y_bin = (highlight_end_y - start) // 128\n",
    "    else :\n",
    "        highlight_start_y_bin = (highlight_start - start) // 128\n",
    "        highlight_end_y_bin = (highlight_end - start) // 128\n",
    "\n",
    "    att[att < vmin] = 0.\n",
    "\n",
    "    f = plt.figure(figsize=figsize)\n",
    "\n",
    "    plt.imshow(att, cmap='hot', vmin=0., vmax=vmax, aspect='equal')\n",
    "\n",
    "    #Draw highlighted rectangle\n",
    "    if highlight_area :\n",
    "        rect = patches.Rectangle(\n",
    "            (highlight_start_x_bin, highlight_start_y_bin),\n",
    "            np.minimum(highlight_end_x_bin-highlight_start_x_bin, 4096-highlight_start_x_bin),\n",
    "            np.minimum(highlight_end_y_bin-highlight_start_y_bin, 4096-highlight_start_y_bin), linewidth=1, edgecolor='magenta', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "\n",
    "    #Draw annotation features\n",
    "    for z_order, annotate_dict in enumerate(annotate_features) :\n",
    "\n",
    "        feature_df = annotation_df.query(\"Feature == '\" + annotate_dict['feature'] + \"'\" + ((\" and \" + annotate_dict['filter_query']) if annotate_dict['filter_query'] is not None and annotate_dict['filter_query'] != \"\" else \"\"))\n",
    "\n",
    "        #Loop over features of the given type\n",
    "        for _, row in feature_df.iterrows() :\n",
    "\n",
    "            start_f = row['Start']\n",
    "            end_f = row['End']\n",
    "\n",
    "            #Plot features of a given length range only\n",
    "            if end_f - start_f < annotate_dict['min_len'] or end_f - start_f > annotate_dict['max_len'] :\n",
    "                continue\n",
    "\n",
    "            start_f_bin = (start_f - start) // 128\n",
    "            end_f_bin = (end_f - start) // 128\n",
    "\n",
    "            #Draw gene text annotation\n",
    "            if annotate_dict['feature'] == 'gene' and annotate_dict['annotate_text'] == True :\n",
    "                if start_f_bin > plot_start_bin and end_f_bin < plot_end_bin :\n",
    "                    plt.gca().text(start_f_bin, end_f_bin + 1, (\"<- \" if row['Strand'] == '-' else \"\") + row['gene_name'] + (\" ->\" if row['Strand'] == '+' else \"\"), ha=\"left\", va=\"bottom\", rotation=0, size=8, color=annotate_dict['color'], fontweight='bold')\n",
    "\n",
    "            #Draw box, line or dots to highlight feature\n",
    "            if annotate_dict['plot_type'] == 'box' :\n",
    "                rect = patches.Rectangle(\n",
    "                    (start_f_bin, start_f_bin),\n",
    "                    np.minimum(end_f_bin-start_f_bin, 4096-start_f_bin),\n",
    "                    np.minimum(end_f_bin-start_f_bin, 4096-start_f_bin), linewidth=2, edgecolor=annotate_dict['color'], facecolor='none')\n",
    "                plt.gca().add_patch(rect)\n",
    "            elif annotate_dict['plot_type'] == 'line' :\n",
    "                plt.plot([start_f_bin, end_f_bin], [start_f_bin, end_f_bin], linewidth=2, color=annotate_dict['color'], zorder=z_order)\n",
    "            elif annotate_dict['plot_type'] == 'dots' :\n",
    "                plt.scatter([start_f_bin], [start_f_bin], marker=annotate_dict['marker'], s=annotate_dict['size'], color=annotate_dict['color'], edgecolor='black', linewidth=0.5, zorder=z_order)\n",
    "                plt.scatter([end_f_bin], [end_f_bin], marker=annotate_dict['marker'], s=annotate_dict['size'], color=annotate_dict['color'], edgecolor='black', linewidth=0.5, zorder=z_order)\n",
    "\n",
    "    z_order = len(annotate_features)\n",
    "\n",
    "    #Plot track densities (y-axis)\n",
    "    plt.gca().fill_between(np.linspace(plot_start_bin, plot_end_bin+track_clip+1, num=yh[plot_start_bin*track_ratio:plot_end_bin*track_ratio].shape[0]+track_clip+1), track_clip + plot_end_bin + 1, y2=plot_end_bin, color='white', zorder=z_order)\n",
    "    plt.gca().fill_between(np.arange((plot_end_bin - plot_start_bin)*track_ratio)/track_ratio + plot_start_bin, np.clip(track_scale * yh[plot_start_bin*track_ratio:plot_end_bin*track_ratio], 0, track_clip) + plot_end_bin, y2=plot_end_bin, color='deepskyblue', zorder=z_order+1)\n",
    "    plt.plot(np.arange((plot_end_bin - plot_start_bin)*track_ratio)/track_ratio + plot_start_bin, np.clip(track_scale * yh[plot_start_bin*track_ratio:plot_end_bin*track_ratio], 0, track_clip) + plot_end_bin, linewidth=1, color='black', zorder=z_order+2)\n",
    "\n",
    "    #Plot densitites (x-axis)\n",
    "    plt.gca().fill_betweenx(np.linspace(plot_start_bin-track_clip, plot_end_bin, num=yh[plot_start_bin*track_ratio:plot_end_bin*track_ratio].shape[0]+track_clip), track_clip + plot_end_bin + 1, x2=plot_end_bin, color='white', zorder=z_order)\n",
    "    plt.gca().fill_betweenx(np.arange((plot_end_bin - plot_start_bin)*track_ratio)/track_ratio + plot_start_bin, np.clip(track_scale * yh[plot_start_bin*track_ratio:plot_end_bin*track_ratio], 0, track_clip) + plot_end_bin, x2=plot_end_bin, color='deepskyblue', zorder=z_order+1)\n",
    "    plt.plot(np.clip(track_scale * yh[plot_start_bin*track_ratio:plot_end_bin*track_ratio], 0, track_clip) + plot_end_bin, np.arange((plot_end_bin - plot_start_bin)*track_ratio)/track_ratio + plot_start_bin, linewidth=1, color='black', zorder=z_order+2)\n",
    "\n",
    "    #Draw bounding frames\n",
    "    plt.plot([track_crop_pooled, track_crop_pooled], [4096, 4096 + track_clip], color='red', linestyle='--', linewidth=1, zorder=z_order+3)\n",
    "    plt.plot([4096 - track_crop_pooled, 4096 - track_crop_pooled], [4096, 4096 + track_clip], color='red', linestyle='--', linewidth=1, zorder=z_order+3)\n",
    "\n",
    "    plt.plot([4096, 4096 + track_clip], [track_crop_pooled, track_crop_pooled], color='red', linestyle='--', linewidth=1, zorder=z_order+3)\n",
    "    plt.plot([4096, 4096 + track_clip], [4096 - track_crop_pooled, 4096 - track_crop_pooled], color='red', linestyle='--', linewidth=1, zorder=z_order+3)\n",
    "\n",
    "    plt.xlim(plot_start_bin, plot_end_bin+track_clip+1)\n",
    "    plt.ylim(plot_start_bin, plot_end_bin+track_clip+1)\n",
    "\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "\n",
    "    #Remove borders\n",
    "    for spine in plt.gca().spines.values():\n",
    "        spine.set_edgecolor('white')\n",
    "\n",
    "    plt.gca().set_xlabel(\"Attended on (\" + chrom + \":\" + str(plot_start) + \"-\" + str(plot_end) + \")  --->\", fontsize=12, loc='left')\n",
    "    plt.gca().set_ylabel(\"Attended by (\" + chrom + \":\" + str(plot_start) + \"-\" + str(plot_end) + \")  --->\", fontsize=12, loc='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_figs :\n",
    "        plt.savefig(\"attention_scores_\" + chrom + \"_\" + str(plot_start) + \"-\" + str(plot_end) + save_suffix + \".png\", dpi=fig_dpi)\n",
    "        plt.savefig(\"attention_scores_\" + chrom + \"_\" + str(plot_start) + \"-\" + str(plot_end) + save_suffix + \".eps\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#Helper function to plot ACGT letters at a given position\n",
    "def dna_letter_at(letter, x, y, yscale=1, ax=None, color=None, alpha=1.0):\n",
    "\n",
    "    fp = FontProperties(family=\"DejaVu Sans\", weight=\"bold\")\n",
    "    globscale = 1.35\n",
    "    LETTERS = {\t\"T\" : TextPath((-0.305, 0), \"T\", size=1, prop=fp),\n",
    "                \"G\" : TextPath((-0.384, 0), \"G\", size=1, prop=fp),\n",
    "                \"A\" : TextPath((-0.35, 0), \"A\", size=1, prop=fp),\n",
    "                \"C\" : TextPath((-0.366, 0), \"C\", size=1, prop=fp),\n",
    "                \"UP\" : TextPath((-0.488, 0), '$\\\\Uparrow$', size=1, prop=fp),\n",
    "                \"DN\" : TextPath((-0.488, 0), '$\\\\Downarrow$', size=1, prop=fp),\n",
    "                \"(\" : TextPath((-0.25, 0), \"(\", size=1, prop=fp),\n",
    "                \".\" : TextPath((-0.125, 0), \"-\", size=1, prop=fp),\n",
    "                \")\" : TextPath((-0.1, 0), \")\", size=1, prop=fp)}\n",
    "    COLOR_SCHEME = {'G': 'orange',#'orange', \n",
    "                    'A': 'green',#'red', \n",
    "                    'C': 'blue',#'blue', \n",
    "                    'T': 'red',#'darkgreen',\n",
    "                    'UP': 'green', \n",
    "                    'DN': 'red',\n",
    "                    '(': 'black',\n",
    "                    '.': 'black', \n",
    "                    ')': 'black'}\n",
    "\n",
    "\n",
    "    text = LETTERS[letter]\n",
    "\n",
    "    chosen_color = COLOR_SCHEME[letter]\n",
    "    if color is not None :\n",
    "        chosen_color = color\n",
    "\n",
    "    t = mpl.transforms.Affine2D().scale(1*globscale, yscale*globscale) + \\\n",
    "        mpl.transforms.Affine2D().translate(x,y) + ax.transData\n",
    "    p = PathPatch(text, lw=0, fc=chosen_color, alpha=alpha, transform=t)\n",
    "    if ax != None:\n",
    "        ax.add_artist(p)\n",
    "    return p\n",
    "\n",
    "#TF function to get average attention gradient\n",
    "def _attention_input_grad(input_sequence, model, att_start_x, att_end_x, att_start_y, att_end_y, subtract_avg) :\n",
    "\n",
    "    #Average slice of attention map\n",
    "    mean_attention = None\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_sequence)\n",
    "        mean_attention = tf.reduce_mean(model(input_sequence)[:, :, att_start_x:att_end_x, att_start_y:att_end_y], axis=(1, 2, 3))\n",
    "\n",
    "    #Calculate gradient\n",
    "    input_grad = tape.gradient(mean_attention, input_sequence)\n",
    "    \n",
    "    #Optionally mean-subtract\n",
    "    if subtract_avg :\n",
    "        input_grad = (input_grad - tf.reduce_mean(input_grad, axis=-1, keepdims=True))\n",
    "    else :\n",
    "        input_grad = input_grad\n",
    "\n",
    "    return input_grad\n",
    "\n",
    "#Function to compute attention gradient on list of sequences (with rc)\n",
    "def get_attention_gradient_w_rc(models, sequence_one_hots, att_start_x, att_end_x, att_start_y, att_end_y, subtract_avg=False, fold_index=[0, 1, 2, 3], layer_index=[5, 6], inital_offset=30, offset=11) :\n",
    "    \n",
    "    #Get gradients for fwd\n",
    "    att_grads = get_attention_gradient(models, sequence_one_hots, att_start_x, att_end_x, att_start_y, att_end_y, subtract_avg, fold_index, layer_index, inital_offset, offset)\n",
    "    \n",
    "    #Get rev sequences\n",
    "    sequence_one_hots_rc = [\n",
    "        sequence_one_hots[example_ix][::-1, ::-1] for example_ix in range(len(sequence_one_hots))\n",
    "    ]\n",
    "    \n",
    "    #Invert attention box\n",
    "    att_start_x = 4096 - att_start_x - 1\n",
    "    att_end_x = 4096 - att_end_x - 1\n",
    "    att_start_y = 4096 - att_start_y - 1\n",
    "    att_end_y = 4096 - att_end_y - 1\n",
    "    \n",
    "    #Get gradients for rev\n",
    "    att_grads_rc = get_attention_gradient(models, sequence_one_hots_rc, att_end_y, att_start_y, att_end_x, att_start_x, subtract_avg, fold_index, layer_index, inital_offset, offset)\n",
    "    \n",
    "    #Average fwd and rev\n",
    "    att_grads_avg = [\n",
    "        (att_grads[example_ix] + att_grads_rc[example_ix][::-1, ::-1]) / 2. for example_ix in range(len(sequence_one_hots))\n",
    "    ]\n",
    "    \n",
    "    return att_grads, att_grads_rc, att_grads_avg\n",
    "\n",
    "#Function to compute attention gradient on list of sequences\n",
    "def get_attention_gradient(models, sequence_one_hots, att_start_x, att_end_x, att_start_y, att_end_y, subtract_avg=False, fold_index=[0, 1, 2, 3], layer_index=[5, 6], inital_offset=30, offset=11) :\n",
    "    \n",
    "    att_grads = np.zeros((len(fold_index), len(layer_index), len(sequence_one_hots), 524288, 4))\n",
    "    \n",
    "    #Loop over folds\n",
    "    for fold_i, fold_ix in enumerate(fold_index) :\n",
    "        \n",
    "        seqnn_model = models[fold_ix]\n",
    "        \n",
    "        #Loop over layers\n",
    "        for layer_i, layer_ix in enumerate(layer_index) :\n",
    "\n",
    "            #Construct attention keras model\n",
    "            attention_model = tf.keras.Model(\n",
    "                seqnn_model.model.layers[1].inputs,\n",
    "                _get_attention_weights(\n",
    "                    seqnn_model.model.layers[1].layers[inital_offset + offset * layer_ix],\n",
    "                    seqnn_model.model.layers[1].layers[inital_offset + offset * layer_ix - 1].output,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            #Define new input tensor\n",
    "            input_sequence = tf.keras.layers.Input(shape=(524288, 4), name='sequence')\n",
    "\n",
    "            #Define lambda layer around attention gradient function\n",
    "            input_grad = tf.keras.layers.Lambda(lambda x: _attention_input_grad(x, attention_model, att_start_x, att_end_x, att_start_y, att_end_y, subtract_avg), name='inp_grad')(input_sequence)\n",
    "\n",
    "            #Create a new keras model that returns the attention gradient\n",
    "            grad_model = tf.keras.models.Model(input_sequence, input_grad)\n",
    "            \n",
    "            #Calculate gradient on CPU\n",
    "            with tf.device('/cpu:0') :\n",
    "                for example_ix in range(len(sequence_one_hots)) :\n",
    "                    att_grads[fold_i, layer_i, example_ix, ...] = sequence_one_hots[example_ix] * grad_model.predict(x=[sequence_one_hots[example_ix][None, ...]], batch_size=1, verbose=True)[0, ...]\n",
    "            \n",
    "            #Run garbage collection before next layer/fold\n",
    "            attention_model = None\n",
    "            gc.collect()\n",
    "    \n",
    "    #Average and input-gate\n",
    "    att_grads = np.mean(att_grads, axis=(0, 1))\n",
    "    att_grads = [\n",
    "        np.sum(att_grads[example_ix, ...], axis=-1, keepdims=True) * sequence_one_hots[example_ix] for example_ix in range(len(sequence_one_hots))\n",
    "    ]\n",
    "    \n",
    "    return att_grads\n",
    "\n",
    "#TF function to get gradient of predictions for a subset of positions and tracks (averaged across tracks)\n",
    "def _prediction_input_grad(input_sequence, model, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, track_index, track_scale, track_transform, clip_soft, use_mean, use_ratio, use_logodds, subtract_avg, prox_bin_index, dist_bin_index) :\n",
    "\n",
    "    mean_dist_prox_ratio = None\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_sequence)\n",
    "        \n",
    "        #Predict and gather selected output tracks\n",
    "        preds = tf.gather(model(input_sequence, training=False), tf.tile(tf.constant(np.array(track_index))[None, :], (tf.shape(input_sequence)[0], 1)), axis=2, batch_dims=1)\n",
    "        \n",
    "        #Undo scale\n",
    "        preds = preds / track_scale\n",
    "\n",
    "        #Undo soft_clip\n",
    "        if clip_soft is not None :\n",
    "            preds = tf.where(preds > clip_soft, (preds - clip_soft)**2 + clip_soft, preds)\n",
    "\n",
    "        #Undo sqrt\n",
    "        preds = preds**(1. / track_transform)\n",
    "      \n",
    "        #Aggregate over tracks (average)\n",
    "        pred = tf.reduce_mean(preds, axis=2)\n",
    "        \n",
    "        #Aggregate over positions and optionally compute ratios\n",
    "        if not use_mean :\n",
    "            #Get sum of distal coverage\n",
    "            if dist_bin_index is None :\n",
    "                mean_dist = tf.reduce_sum(pred[:, dist_bin_start:dist_bin_end], axis=1)\n",
    "            else :\n",
    "                mean_dist = tf.reduce_sum(tf.gather(pred, dist_bin_index, axis=1), axis=1)\n",
    "            \n",
    "            #Get sum of proximal coverage\n",
    "            if prox_bin_index is None :\n",
    "                mean_prox = tf.reduce_sum(pred[:, prox_bin_start:prox_bin_end], axis=1)\n",
    "            else :\n",
    "                mean_prox = tf.reduce_sum(tf.gather(pred, prox_bin_index, axis=1), axis=1)\n",
    "        else :\n",
    "            #Get mean distal coverage\n",
    "            if dist_bin_index is None :\n",
    "                mean_dist = tf.reduce_mean(pred[:, dist_bin_start:dist_bin_end], axis=1)\n",
    "            else :\n",
    "                mean_dist = tf.reduce_mean(tf.gather(pred, dist_bin_index, axis=1), axis=1)\n",
    "            \n",
    "            #Get mean proximal coverage\n",
    "            if prox_bin_index is None :\n",
    "                mean_prox = tf.reduce_mean(pred[:, prox_bin_start:prox_bin_end], axis=1)\n",
    "            else :\n",
    "                mean_prox = tf.reduce_mean(tf.gather(pred, prox_bin_index, axis=1), axis=1)\n",
    "        if not use_ratio :\n",
    "            #Get log-sum of coverage\n",
    "            mean_dist_prox_ratio = tf.math.log(mean_dist + 1e-6)\n",
    "        else :\n",
    "            #Get ratio (or log odds) of distal and proximal coverage samples\n",
    "            if not use_logodds :\n",
    "                mean_dist_prox_ratio = tf.math.log(mean_dist / mean_prox + 1e-6)\n",
    "            else :\n",
    "                mean_dist_prox_ratio = tf.math.log((mean_dist / mean_prox) / (1. - (mean_dist / mean_prox)) + 1e-6)\n",
    "\n",
    "    #Calculate gradient and optionally mean-subtract\n",
    "    input_grad = tape.gradient(mean_dist_prox_ratio, input_sequence)\n",
    "    if subtract_avg :\n",
    "        input_grad = (input_grad - tf.reduce_mean(input_grad, axis=-1, keepdims=True))\n",
    "    else :\n",
    "        input_grad = input_grad\n",
    "\n",
    "    return input_grad\n",
    "\n",
    "#Function to compute prediction gradients for list of sequences (with rc)\n",
    "def get_prediction_gradient_w_rc(models, sequence_one_hots, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, track_index, track_scale, track_transform, clip_soft=None, prox_bin_index=None, dist_bin_index=None, use_mean=False, use_ratio=True, use_logodds=False, subtract_avg=False, fold_index=[0, 1, 2, 3]) :\n",
    "    \n",
    "    #Get gradients for fwd\n",
    "    pred_grads = get_prediction_gradient(models, sequence_one_hots, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, track_index, track_scale, track_transform, clip_soft, prox_bin_index, dist_bin_index, use_mean, use_ratio, use_logodds, subtract_avg, fold_index)\n",
    "    \n",
    "    #Get sequences for rev\n",
    "    sequence_one_hots_rc = [\n",
    "        sequence_one_hots[example_ix][::-1, ::-1] for example_ix in range(len(sequence_one_hots))\n",
    "    ]\n",
    "    \n",
    "    #Invert start/end of distal and proximal bins\n",
    "    prox_bin_start_rc = models[0].target_lengths[0] - prox_bin_start - 1\n",
    "    prox_bin_end_rc = models[0].target_lengths[0] - prox_bin_end - 1\n",
    "    \n",
    "    dist_bin_start_rc = models[0].target_lengths[0] - dist_bin_start - 1\n",
    "    dist_bin_end_rc = models[0].target_lengths[0] - dist_bin_end - 1\n",
    "    \n",
    "    #Invert bin index vectors\n",
    "    prox_bin_index_rc = None\n",
    "    if prox_bin_index is not None :\n",
    "        prox_bin_index_rc = [models[0].target_lengths[0] - prox_bin - 1 for prox_bin in prox_bin_index]\n",
    "    \n",
    "    dist_bin_index_rc = None\n",
    "    if dist_bin_index is not None :\n",
    "        dist_bin_index_rc = [models[0].target_lengths[0] - dist_bin - 1 for dist_bin in dist_bin_index]\n",
    "    \n",
    "    #Get gradients for rev\n",
    "    pred_grads_rc = get_prediction_gradient(models, sequence_one_hots_rc, prox_bin_end_rc, prox_bin_start_rc, dist_bin_end_rc, dist_bin_start_rc, track_index, track_scale, track_transform, clip_soft, prox_bin_index_rc, dist_bin_index_rc, use_mean, use_ratio, use_logodds, subtract_avg, fold_index)\n",
    "    \n",
    "    pred_grads_avg = [\n",
    "        (pred_grads[example_ix] + pred_grads_rc[example_ix][::-1, ::-1]) / 2. for example_ix in range(len(sequence_one_hots))\n",
    "    ]\n",
    "    \n",
    "    return pred_grads, pred_grads_rc, pred_grads_avg\n",
    "\n",
    "#Function to compute prediction gradients for list of sequences\n",
    "def get_prediction_gradient(models, sequence_one_hots, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, track_index, track_scale, track_transform, clip_soft=None, prox_bin_index=None, dist_bin_index=None, use_mean=False, use_ratio=True, use_logodds=False, subtract_avg=False, fold_index=[0, 1, 2, 3]) :\n",
    "    \n",
    "    pred_grads = np.zeros((len(sequence_one_hots), len(fold_index), 524288, 4))\n",
    "    \n",
    "    #Loop over folds\n",
    "    for fold_i, fold_ix in enumerate(fold_index) :\n",
    "        \n",
    "        prediction_model = models[fold_ix].model.layers[1]\n",
    "        \n",
    "        #Create new input tensor\n",
    "        input_sequence = tf.keras.layers.Input(shape=(524288, 4), name='sequence')\n",
    "\n",
    "        #Wrap prediction gradient TF function in a lambda layer\n",
    "        input_grad = tf.keras.layers.Lambda(lambda x: _prediction_input_grad(x, prediction_model, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, track_index, track_scale, track_transform, clip_soft, use_mean, use_ratio, use_logodds, subtract_avg, prox_bin_index, dist_bin_index), name='inp_grad')(input_sequence)\n",
    "\n",
    "        #Create new keras model that returns the gradient of predictions\n",
    "        grad_model = tf.keras.models.Model(input_sequence, input_grad)\n",
    "        \n",
    "        #Compute gradient on CPU\n",
    "        with tf.device('/cpu:0') :\n",
    "            for example_ix in range(len(sequence_one_hots)) :\n",
    "                pred_grads[example_ix, fold_i, ...] = sequence_one_hots[example_ix] * grad_model.predict(x=[sequence_one_hots[example_ix][None, ...]], batch_size=1, verbose=True)[0, ...]\n",
    "        \n",
    "        #Run garbage collection before next fold\n",
    "        prediction_model = None\n",
    "        gc.collect()\n",
    "    \n",
    "    #Average and mean-subtract\n",
    "    pred_grads = np.mean(pred_grads, axis=1)\n",
    "    pred_grads = [\n",
    "        np.sum(pred_grads[example_ix, ...], axis=-1, keepdims=True) * sequence_one_hots[example_ix] for example_ix in range(len(sequence_one_hots))\n",
    "    ]\n",
    "    \n",
    "    return pred_grads\n",
    "\n",
    "#Function to calculate prediction statistic for a subset of positions\n",
    "def _prediction_ism_score(pred, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, use_mean, use_ratio, use_logodds, prox_bin_index, dist_bin_index) :\n",
    "\n",
    "    #Aggregate over positions and optionally compute ratios\n",
    "    if not use_mean :\n",
    "        #Get sum of distal coverage\n",
    "        if dist_bin_index is None :\n",
    "            mean_dist = np.sum(pred[:, dist_bin_start:dist_bin_end], axis=1)\n",
    "        else :\n",
    "            mean_dist = np.sum(pred[:, dist_bin_index], axis=1)\n",
    "        \n",
    "        #Get mean proximal coverage\n",
    "        if prox_bin_index is None :\n",
    "            mean_prox = np.sum(pred[:, prox_bin_start:prox_bin_end], axis=1)\n",
    "        else :\n",
    "            mean_prox = np.sum(pred[:, prox_bin_index], axis=1)\n",
    "    else:\n",
    "        #Get sum of distal coverage\n",
    "        if dist_bin_index is None :\n",
    "            mean_dist = np.mean(pred[:, dist_bin_start:dist_bin_end], axis=1)\n",
    "        else :\n",
    "            mean_dist = np.mean(pred[:, dist_bin_index], axis=1)\n",
    "        \n",
    "        #Get mean proximal coverage\n",
    "        if prox_bin_index is None :\n",
    "            mean_prox = np.mean(pred[:, prox_bin_start:prox_bin_end], axis=1)\n",
    "        else :\n",
    "            mean_prox = np.mean(pred[:, prox_bin_index], axis=1)\n",
    "    \n",
    "    if not use_ratio :\n",
    "        #Get log-sum of coverage\n",
    "        mean_dist_prox_ratio = np.log(mean_dist + 1e-6)\n",
    "    else :\n",
    "        #Get ratio (or log odds) of distal and proximal coverage samples\n",
    "        if not use_logodds :\n",
    "            mean_dist_prox_ratio = np.log(mean_dist / mean_prox + 1e-6)\n",
    "        else :\n",
    "            mean_dist_prox_ratio = np.log((mean_dist / mean_prox) / (1. - (mean_dist / mean_prox)) + 1e-6)\n",
    "\n",
    "    return mean_dist_prox_ratio\n",
    "\n",
    "#Function to compute ISM scores over a range of positions, for a list of sequences\n",
    "def get_ism(models, sequence_one_hots, ism_start, ism_end, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, track_index, track_scale, track_transform, clip_soft, prox_bin_index=None, dist_bin_index=None, use_mean=False, use_ratio=True, use_logodds=False) :\n",
    "    \n",
    "    pred_ism = np.zeros((len(sequence_one_hots), len(models), 524288, 4))\n",
    "    \n",
    "    bases = [0, 1, 2, 3]\n",
    "    \n",
    "    #Loop over sequences\n",
    "    for example_ix in range(len(sequence_one_hots)) :\n",
    "        \n",
    "        print(\"example_ix = \" + str(example_ix))\n",
    "        \n",
    "        sequence_one_hot_wt = sequence_one_hots[example_ix]\n",
    "        \n",
    "        #Get pred\n",
    "        y_wt = predict_tracks(models, sequence_one_hot_wt)[0, ...][..., track_index].astype('float32')\n",
    "        \n",
    "        #Undo scale\n",
    "        y_wt /= track_scale\n",
    "\n",
    "        #Undo soft_clip\n",
    "        if clip_soft is not None :\n",
    "            y_wt_unclipped = (y_wt - clip_soft)**2 + clip_soft\n",
    "            unclip_mask_wt = (y_wt > clip_soft)\n",
    "\n",
    "            y_wt[unclip_mask_wt] = y_wt_unclipped[unclip_mask_wt]\n",
    "\n",
    "        #Undo sqrt\n",
    "        y_wt = y_wt**(1. / track_transform)\n",
    "        \n",
    "        #Aggregate over tracks (average)\n",
    "        y_wt = np.mean(y_wt, axis=-1)\n",
    "        \n",
    "        #Compute scalar statistics for wt sequence\n",
    "        score_wt = _prediction_ism_score(y_wt, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, use_mean, use_ratio, use_logodds, prox_bin_index, dist_bin_index)\n",
    "        \n",
    "        #Loop over positions in range\n",
    "        for j in range(ism_start, ism_end) :\n",
    "            for b in bases :\n",
    "                #Mutate sequence\n",
    "                if sequence_one_hot_wt[j, b] != 1. : \n",
    "                    sequence_one_hot_mut = np.copy(sequence_one_hot_wt)\n",
    "                    sequence_one_hot_mut[j, :] = 0.\n",
    "                    sequence_one_hot_mut[j, b] = 1.\n",
    "                    \n",
    "                    #Get pred\n",
    "                    y_mut = predict_tracks(models, sequence_one_hot_mut)[0, ...][..., track_index].astype('float32')\n",
    "\n",
    "                    #Undo scale\n",
    "                    y_mut /= track_scale\n",
    "\n",
    "                    #Undo soft_clip\n",
    "                    if clip_soft is not None :\n",
    "                        y_mut_unclipped = (y_mut - clip_soft)**2 + clip_soft\n",
    "                        unclip_mask_mut = (y_mut > clip_soft)\n",
    "\n",
    "                        y_mut[unclip_mask_mut] = y_mut_unclipped[unclip_mask_mut]\n",
    "\n",
    "                    #Undo sqrt\n",
    "                    y_mut = y_mut**(1. / track_transform)\n",
    "\n",
    "                    #Aggregate over tracks (average)\n",
    "                    y_mut = np.mean(y_mut, axis=-1)\n",
    "                    \n",
    "                    #Compute scalar statistic for mutated sequence\n",
    "                    score_mut = _prediction_ism_score(y_mut, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, use_mean, use_ratio, use_logodds, prox_bin_index, dist_bin_index)\n",
    "                    \n",
    "                    pred_ism[example_ix, :, j, b] = score_wt - score_mut\n",
    "        \n",
    "        #Average across nucleotides\n",
    "        pred_ism[example_ix, ...] = np.tile(np.mean(pred_ism[example_ix, ...], axis=-1)[..., None], (1, 1, 4)) * sequence_one_hots[example_ix][None, ...]\n",
    "\n",
    "    #Average across folds\n",
    "    pred_ism = np.mean(pred_ism, axis=1)\n",
    "    pred_ism = [pred_ism[example_ix, ...] for example_ix in range(len(sequence_one_hots))]\n",
    "    \n",
    "    return pred_ism\n",
    "\n",
    "#Function to compute ISM shuffle scores over a range of positions, for a list of sequences\n",
    "def get_ism_shuffle(models, sequence_one_hots, ism_start, ism_end, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, track_index, track_scale, track_transform, clip_soft, prox_bin_index=None, dist_bin_index=None, window_size=5, n_samples=8, mononuc_shuffle=False, dinuc_shuffle=False, use_mean=False, use_ratio=True, use_logodds=False) :\n",
    "    \n",
    "    pred_shuffle = np.zeros((len(sequence_one_hots), len(models), 524288, n_samples))\n",
    "    pred_ism = np.zeros((len(sequence_one_hots), len(models), 524288, 4))\n",
    "    \n",
    "    bases = [0, 1, 2, 3]\n",
    "    \n",
    "    #Loop over sequences\n",
    "    for example_ix in range(len(sequence_one_hots)) :\n",
    "        \n",
    "        print(\"example_ix = \" + str(example_ix))\n",
    "        \n",
    "        sequence_one_hot_wt = sequence_one_hots[example_ix]\n",
    "    \n",
    "        #Get pred\n",
    "        y_wt = predict_tracks(models, sequence_one_hot_wt)[0, ...][..., track_index].astype('float32')\n",
    "        \n",
    "        #Undo scale\n",
    "        y_wt /= track_scale\n",
    "\n",
    "        #Undo soft_clip\n",
    "        if clip_soft is not None :\n",
    "            y_wt_unclipped = (y_wt - clip_soft)**2 + clip_soft\n",
    "            unclip_mask_wt = (y_wt > clip_soft)\n",
    "\n",
    "            y_wt[unclip_mask_wt] = y_wt_unclipped[unclip_mask_wt]\n",
    "\n",
    "        #Undo sqrt\n",
    "        y_wt = y_wt**(1. / track_transform)\n",
    "        \n",
    "        #Aggregate over tracks (average)\n",
    "        y_wt = np.mean(y_wt, axis=-1)\n",
    "        \n",
    "        #Compute scalar statistics for wt sequence\n",
    "        score_wt = _prediction_ism_score(y_wt, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, use_mean, use_ratio, use_logodds, prox_bin_index, dist_bin_index)\n",
    "        \n",
    "        #Loop over ISM position range\n",
    "        for j in range(ism_start, ism_end) :\n",
    "            \n",
    "            j_start = j - window_size // 2\n",
    "            j_end = j + window_size // 2 + 1\n",
    "            \n",
    "            pos_index = np.arange(j_end-j_start)+j_start\n",
    "            \n",
    "            #Loop over samples at position j\n",
    "            for sample_ix in range(n_samples) :\n",
    "                sequence_one_hot_mut = np.copy(sequence_one_hot_wt)\n",
    "                sequence_one_hot_mut[j_start:j_end, :] = 0.\n",
    "                \n",
    "                #Randomly mutate\n",
    "                if not mononuc_shuffle and not dinuc_shuffle :\n",
    "                    nt_index = np.random.choice(bases, size=(j_end-j_start,)).tolist()\n",
    "                    sequence_one_hot_mut[pos_index, nt_index] = 1.\n",
    "                elif mononuc_shuffle :\n",
    "                    #Shuffle nucleotides\n",
    "                    shuffled_pos_index = np.copy(pos_index)\n",
    "                    np.random.shuffle(shuffled_pos_index)\n",
    "                    \n",
    "                    sequence_one_hot_mut[shuffled_pos_index, :] = sequence_one_hot_wt[pos_index, :]\n",
    "                else : #Dinucleotide-shuffle\n",
    "                    if sample_ix % 2 == 0 :\n",
    "                        shuffled_pos_index = [\n",
    "                            [pos_index[pos_j], pos_index[pos_j+1]] if pos_j+1 < pos_index.shape[0] else [pos_index[pos_j]]\n",
    "                            for pos_j in range(0, pos_index.shape[0], 2)\n",
    "                        ]\n",
    "                    else : #Offset sequence by 1 to get other set of dinucleotides\n",
    "                        pos_index_rev = np.copy(pos_index)[::-1]\n",
    "                        shuffled_pos_index = [\n",
    "                            [pos_index_rev[pos_j], pos_index_rev[pos_j+1]] if pos_j+1 < pos_index_rev.shape[0] else [pos_index_rev[pos_j]]\n",
    "                            for pos_j in range(0, pos_index_rev.shape[0], 2)\n",
    "                        ]\n",
    "                    \n",
    "                    shuffled_shuffle_index = np.arange(len(shuffled_pos_index), dtype='int32')\n",
    "                    np.random.shuffle(shuffled_shuffle_index)\n",
    "                    \n",
    "                    #Shuffle position indices\n",
    "                    shuffled_pos_index_new = []\n",
    "                    for pos_tuple_i in range(len(shuffled_pos_index)) :\n",
    "                        shuffled_pos_index_new.extend(shuffled_pos_index[shuffled_shuffle_index[pos_tuple_i]])\n",
    "                    \n",
    "                    #Apply shuffling\n",
    "                    shuffled_pos_index = np.array(shuffled_pos_index_new, dtype='int32')\n",
    "                    sequence_one_hot_mut[shuffled_pos_index, :] = sequence_one_hot_wt[pos_index, :]\n",
    "\n",
    "                #Get pred\n",
    "                y_mut = predict_tracks(models, sequence_one_hot_mut)[0, ...][..., track_index].astype('float32')\n",
    "\n",
    "                #Undo scale\n",
    "                y_mut /= track_scale\n",
    "\n",
    "                #Undo soft_clip\n",
    "                if clip_soft is not None :\n",
    "                    y_mut_unclipped = (y_mut - clip_soft)**2 + clip_soft\n",
    "                    unclip_mask_mut = (y_mut > clip_soft)\n",
    "\n",
    "                    y_mut[unclip_mask_mut] = y_mut_unclipped[unclip_mask_mut]\n",
    "\n",
    "                #Undo sqrt\n",
    "                y_mut = y_mut**(1. / track_transform)\n",
    "\n",
    "                #Aggregate over tracks (average)\n",
    "                y_mut = np.mean(y_mut, axis=-1)\n",
    "                \n",
    "                #Compute scalar statistics for mutated sequence\n",
    "                score_mut = _prediction_ism_score(y_mut, prox_bin_start, prox_bin_end, dist_bin_start, dist_bin_end, use_mean, use_ratio, use_logodds, prox_bin_index, dist_bin_index)\n",
    "\n",
    "                pred_shuffle[example_ix, :, j, sample_ix] = score_wt - score_mut\n",
    "\n",
    "        pred_ism[example_ix, ...] = np.tile(np.mean(pred_shuffle[example_ix, ...], axis=-1)[..., None], (1, 1, 4)) * sequence_one_hots[example_ix][None, ...]\n",
    "\n",
    "    pred_ism = np.mean(pred_ism, axis=1)\n",
    "    pred_ism = [pred_ism[example_ix, ...] for example_ix in range(len(sequence_one_hots))]\n",
    "    \n",
    "    return pred_ism\n",
    "\n",
    "#Function to plot sequence logo\n",
    "def plot_seq_scores(importance_scores, figsize=(16, 2), plot_y_ticks=True, y_min=None, y_max=None, save_figs=False, fig_name=\"default\") :\n",
    "\n",
    "    importance_scores = importance_scores.T\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    ref_seq = \"\"\n",
    "    \n",
    "    #Loop over reference sequence letters\n",
    "    for j in range(importance_scores.shape[1]) :\n",
    "        argmax_nt = np.argmax(np.abs(importance_scores[:, j]))\n",
    "        \n",
    "        if argmax_nt == 0 :\n",
    "            ref_seq += \"A\"\n",
    "        elif argmax_nt == 1 :\n",
    "            ref_seq += \"C\"\n",
    "        elif argmax_nt == 2 :\n",
    "            ref_seq += \"G\"\n",
    "        elif argmax_nt == 3 :\n",
    "            ref_seq += \"T\"\n",
    "\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    #Loop over reference sequence letters and draw\n",
    "    for i in range(0, len(ref_seq)) :\n",
    "        mutability_score = np.sum(importance_scores[:, i])\n",
    "        color = None\n",
    "        dna_letter_at(ref_seq[i], i + 0.5, 0, mutability_score, ax, color=color)\n",
    "    \n",
    "    plt.sca(ax)\n",
    "    plt.xticks([], [])\n",
    "    plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "    \n",
    "    plt.xlim((0, len(ref_seq)))\n",
    "    \n",
    "    #plt.axis('off')\n",
    "    \n",
    "    if plot_y_ticks :\n",
    "        plt.yticks(fontsize=12)\n",
    "    else :\n",
    "        plt.yticks([], [])\n",
    "    \n",
    "    #Set axis limits\n",
    "    if y_min is not None and y_max is not None :\n",
    "        plt.ylim(y_min, y_max)\n",
    "    elif y_min is not None :\n",
    "        plt.ylim(y_min)\n",
    "    else :\n",
    "        plt.ylim(\n",
    "            np.min(importance_scores) - 0.1 * np.max(np.abs(importance_scores)),\n",
    "            np.max(importance_scores) + 0.1 * np.max(np.abs(importance_scores))\n",
    "        )\n",
    "    \n",
    "    plt.axhline(y=0., color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "    #for axis in fig.axes :\n",
    "    #    axis.get_xaxis().set_visible(False)\n",
    "    #    axis.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_figs :\n",
    "        plt.savefig(fig_name + \".png\", transparent=True, dpi=300)\n",
    "        plt.savefig(fig_name + \".eps\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#Plot pair of sequence logos with matched y-axis height\n",
    "def visualize_input_gradient_pair(att_grad_wt, att_grad_mut, plot_start=0, plot_end=100, save_figs=False, fig_name='') :\n",
    "\n",
    "    scores_wt = att_grad_wt[plot_start:plot_end, :]\n",
    "    scores_mut = att_grad_mut[plot_start:plot_end, :]\n",
    "\n",
    "    #Get logo bounds\n",
    "    y_min = min(np.min(scores_wt), np.min(scores_mut))\n",
    "    y_max = max(np.max(scores_wt), np.max(scores_mut))\n",
    "\n",
    "    y_max_abs = max(np.abs(y_min), np.abs(y_max))\n",
    "\n",
    "    y_min = y_min - 0.05 * y_max_abs\n",
    "    y_max = y_max + 0.05 * y_max_abs\n",
    "    \n",
    "    print(\"y_min = \" + str(round(y_min, 8)))\n",
    "    print(\"y_max = \" + str(round(y_max, 8)))\n",
    "\n",
    "    #Plot wt logo\n",
    "    print(\"--- WT ---\")\n",
    "    plot_seq_scores(\n",
    "        scores_wt, y_min=y_min, y_max=y_max,\n",
    "        figsize=(8, 1),\n",
    "        plot_y_ticks=False,\n",
    "        save_figs=save_figs,\n",
    "        fig_name=fig_name + '_wt',\n",
    "    )\n",
    "\n",
    "    #Plot mut logo\n",
    "    print(\"--- Mut ---\")\n",
    "    plot_seq_scores(\n",
    "        scores_mut, y_min=y_min, y_max=y_max,\n",
    "        figsize=(8, 1),\n",
    "        plot_y_ticks=False,\n",
    "        save_figs=save_figs,\n",
    "        fig_name=fig_name + '_mut',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize fasta sequence extractor\n",
    "import pyfaidx\n",
    "import pysam\n",
    "fasta_file = '/home/gl/projects/Borzoi/borzoi/examples/hg38/assembly/ucsc/hg38.fa'\n",
    "\n",
    "#!wget -O - http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz | gunzip -c > {fasta_file}\n",
    "pyfaidx.Faidx(fasta_file)\n",
    "\n",
    "fasta_open = pysam.Fastafile(fasta_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define default annotation settings\n",
    "\n",
    "annotate_features = [\n",
    "    {'feature' : 'gene', 'annotate_text' : True, 'filter_query' : \"Strand == '+'\", 'plot_type' : 'box', 'color' : 'lightgreen', 'marker' : None, 'min_len' : 4096, 'max_len' : 1e9},\n",
    "    {'feature' : 'gene', 'annotate_text' : True, 'filter_query' : \"Strand == '-'\", 'plot_type' : 'box', 'color' : 'deepskyblue', 'marker' : None, 'min_len' : 4096, 'max_len' : 1e9},\n",
    "    \n",
    "    {'feature' : 'five_prime_UTR', 'annotate_text' : False, 'filter_query' : None, 'plot_type' : 'line', 'color' : 'magenta', 'marker' : None, 'min_len' : 0, 'max_len' : 1e9},\n",
    "    {'feature' : 'three_prime_UTR', 'annotate_text' : False, 'filter_query' : None, 'plot_type' : 'line', 'color' : 'magenta', 'marker' : None, 'min_len' : 0, 'max_len' : 1e9},\n",
    "    \n",
    "    {'feature' : 'exon', 'annotate_text' : False, 'filter_query' : None, 'plot_type' : 'line', 'color' : 'deepskyblue', 'marker' : None, 'min_len' : 0, 'max_len' : 1e9},\n",
    "    \n",
    "    {'feature' : 'exon', 'annotate_text' : False, 'filter_query' : None, 'plot_type' : 'dots', 'color' : 'deepskyblue', 'marker' : \"*\", 'size' : 50, 'min_len' : 0, 'max_len' : 1e9},\n",
    "    {'feature' : 'polyA_site', 'annotate_text' : False, 'filter_query' : None, 'plot_type' : 'dots', 'color' : 'red', 'marker' : \"*\", 'size' : 50, 'min_len' : 0, 'max_len' : 1e9},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_folds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m70517808\u001b[39m\n\u001b[1;32m      7\u001b[0m sequence_one_hot, annotation_df \u001b[38;5;241m=\u001b[39m process_sequence(chrom, start, end)\n\u001b[0;32m----> 9\u001b[0m predicted_tracks, attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_tracks_and_attention_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_one_hot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_rc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 109\u001b[0m, in \u001b[0;36mpredict_tracks_and_attention_scores\u001b[0;34m(models, sequence_one_hot, track_scale, track_transform, clip_soft, n_layers, score_rc)\u001b[0m\n\u001b[1;32m    106\u001b[0m predicted_tracks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m#Loop over folds\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_ix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mn_folds\u001b[49m) :\n\u001b[1;32m    111\u001b[0m     attention_scores_for_fold \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    113\u001b[0m     yh \u001b[38;5;241m=\u001b[39m models[fold_ix](sequence_one_hot[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m])[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_folds' is not defined"
     ]
    }
   ],
   "source": [
    "#Predict for example sequence chr1:69993520-70517808\n",
    "\n",
    "chrom = 'chr1'\n",
    "start = 69993520\n",
    "end = 70517808\n",
    "\n",
    "sequence_one_hot, annotation_df = process_sequence(chrom, start, end)\n",
    "\n",
    "predicted_tracks, attention_scores = predict_tracks_and_attention_scores(models, sequence_one_hot, score_rc=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_attention_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Viualize scores for sequence chr1:69993520-70517808 (with annotation)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplot_attention_score\u001b[49m(\n\u001b[1;32m      4\u001b[0m     predicted_tracks,\n\u001b[1;32m      5\u001b[0m     attention_scores,\n\u001b[1;32m      6\u001b[0m     chrom\u001b[38;5;241m=\u001b[39mchrom,\n\u001b[1;32m      7\u001b[0m     start\u001b[38;5;241m=\u001b[39mstart,\n\u001b[1;32m      8\u001b[0m     end\u001b[38;5;241m=\u001b[39mend,\n\u001b[1;32m      9\u001b[0m     track_crop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     10\u001b[0m     track_pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     11\u001b[0m     plot_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m69993520\u001b[39m,\n\u001b[1;32m     12\u001b[0m     plot_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m70517808\u001b[39m,\n\u001b[1;32m     13\u001b[0m     save_suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_pooled\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m     annotate_features\u001b[38;5;241m=\u001b[39mannotate_features,\n\u001b[1;32m     15\u001b[0m     save_figs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m     track_scale_qtl\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.98\u001b[39m\n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_attention_score' is not defined"
     ]
    }
   ],
   "source": [
    "#Viualize scores for sequence chr1:69993520-70517808 (with annotation)\n",
    "\n",
    "plot_attention_score(\n",
    "    predicted_tracks,\n",
    "    attention_scores,\n",
    "    chrom=chrom,\n",
    "    start=start,\n",
    "    end=end,\n",
    "    track_crop=16,\n",
    "    track_pool=4,\n",
    "    plot_start=69993520,\n",
    "    plot_end=70517808,\n",
    "    save_suffix='_pooled',\n",
    "    annotate_features=annotate_features,\n",
    "    save_figs=True,\n",
    "    track_scale_qtl=0.98\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict for example sequence chr1:69967222-70491510 (re-centered on SRSF11), undo transforms\n",
    "\n",
    "chrom = 'chr1'\n",
    "start = 69967222\n",
    "end = 70491510\n",
    "\n",
    "point_start = 70221710\n",
    "\n",
    "sequence_one_hot_wt, annotation_df = process_sequence(chrom, start, end)\n",
    "\n",
    "predicted_tracks_wt, attention_scores_wt = predict_tracks_and_attention_scores(models, sequence_one_hot_wt, track_scale=0.01, track_transform=3./4., clip_soft=384., score_rc=True)\n",
    "\n",
    "#Also collect original (transformed) predictions\n",
    "##predicted_tracks_wt_orig, _ = predict_tracks_and_attention_scores(models, sequence_one_hot_wt, score_rc=True)\n",
    "\n",
    "#Shuffle CRE region while preserving dinucleotide content\n",
    "\n",
    "#Set start and end positions for local dinuc shuffle\n",
    "shuffle_start = point_start - start - 3 * 128\n",
    "shuffle_end = point_start - start - 1 * 128\n",
    "\n",
    "#Copy wt sequence\n",
    "sequence_one_hot_mut = np.copy(sequence_one_hot_wt)\n",
    "\n",
    "#Get vector of positions to shuffle\n",
    "pos_index = np.arange(shuffle_end-shuffle_start)+shuffle_start\n",
    "\n",
    "#Clear positions in mutated sequence\n",
    "sequence_one_hot_mut[shuffle_start:shuffle_end, :] = 0.\n",
    "\n",
    "#Get list of dinucleotide positions\n",
    "shuffled_pos_index = [\n",
    "    [pos_index[pos_j], pos_index[pos_j+1]]\n",
    "    for pos_j in range(0, pos_index.shape[0], 2)\n",
    "]\n",
    "\n",
    "#Get shuffle permutation index\n",
    "\n",
    "#Set seed\n",
    "np.random.seed(1234)\n",
    "\n",
    "shuffled_shuffle_index = np.arange(len(shuffled_pos_index), dtype='int32')\n",
    "np.random.shuffle(shuffled_shuffle_index)\n",
    "\n",
    "#Reset seed\n",
    "np.random.seed()\n",
    "\n",
    "#Shuffle dinucleotide positions and flatten into nucleotide position index\n",
    "shuffled_pos_index_new = []\n",
    "for pos_tuple_i in range(len(shuffled_pos_index)) :\n",
    "    shuffled_pos_index_new.extend(shuffled_pos_index[shuffled_shuffle_index[pos_tuple_i]])\n",
    "\n",
    "#Shuffle nucleotides according to position index\n",
    "shuffled_pos_index = np.array(shuffled_pos_index_new, dtype='int32')\n",
    "sequence_one_hot_mut[shuffled_pos_index, :] = sequence_one_hot_wt[pos_index, :]\n",
    "\n",
    "predicted_tracks_mut, attention_scores_mut = predict_tracks_and_attention_scores(models, sequence_one_hot_mut, track_scale=0.01, track_transform=3./4., clip_soft=384., score_rc=True)\n",
    "\n",
    "#Also collect original (transformed) predictions\n",
    "##predicted_tracks_mut_orig, _ = predict_tracks_and_attention_scores(models, sequence_one_hot_mut, score_rc=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Place bounding box over gene attention stripe\n",
    "\n",
    "bin_size = 128\n",
    "\n",
    "attention_box_start_x = point_start - 4 * bin_size\n",
    "attention_box_end_x = point_start + 1 * bin_size\n",
    "\n",
    "attention_box_start_y = point_start + 50 * bin_size\n",
    "attention_box_end_y = point_start + 250 * bin_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get brain, lung and blood targets (relative indices)\n",
    "\n",
    "targets_df['row_index'] = np.arange(len(targets_df), dtype='int32')\n",
    "\n",
    "print(\"brain = \" + str(targets_df.loc[targets_df['description'] == 'RNA:brain']['row_index'].values.tolist()))\n",
    "print(\"lung = \" + str(targets_df.loc[targets_df['description'] == 'RNA:lung']['row_index'].values.tolist()))\n",
    "print(\"blood = \" + str(targets_df.loc[targets_df['description'] == 'RNA:blood']['row_index'].values.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute and print predicted coverage fold changes over gene SRSF11 (TSS2)\n",
    "\n",
    "bin_start = (70221700 - start) // 32 - 16\n",
    "bin_end = (70255052 - start) // 32 - 16\n",
    "\n",
    "wt_sum = np.sum(np.mean(predicted_tracks_wt[0, :, bin_start:bin_end, :], axis=0), axis=0)\n",
    "mut_sum = np.sum(np.mean(predicted_tracks_mut[0, :, bin_start:bin_end, :], axis=0), axis=0)\n",
    "\n",
    "mut_fc = mut_sum / wt_sum\n",
    "\n",
    "print(\" - average fold change (all tissues) = \" + str(round(np.mean(mut_fc), 4)))\n",
    "print(\"\")\n",
    "\n",
    "print(\" - sorted in decreasing order of coverage loss - \")\n",
    "\n",
    "target_names = sorted(targets_df['description'].unique().tolist())\n",
    "target_fcs = []\n",
    "target_wts = []\n",
    "\n",
    "# loop over targets\n",
    "for target_name in target_names :\n",
    "    \n",
    "    target_list = targets_df.loc[targets_df['description'] == target_name]['row_index'].values.tolist()\n",
    "    target_fcs.append(round(np.mean(mut_fc[target_list]), 4))\n",
    "    target_wts.append(round(np.mean(wt_sum[target_list]), 4))\n",
    "\n",
    "target_fc = np.array(target_fcs, dtype='float32')\n",
    "target_wt = np.array(target_wts, dtype='float32')\n",
    "\n",
    "# loop over targets again (print in sorted order)\n",
    "sort_index = np.argsort(target_fc)\n",
    "for i in sort_index :\n",
    "    \n",
    "    target_list = targets_df.loc[targets_df['description'] == target_names[i]]['row_index'].values.tolist()\n",
    "    print(str(target_fc[i]) + \" (\" + target_names[i] + \"; \" + str(target_list) + \"; \" + str(target_wt[i]) + \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize attention map and predicted coverage (SRSF11 window, raw transformed tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viualize scores for sequence chr1:70203681-70255052 (wildtype)\n",
    "\n",
    "plot_attention_score(\n",
    "    np.mean(predicted_tracks_wt_orig[..., :], axis=-1),\n",
    "    attention_scores_wt,\n",
    "    chrom=chrom,\n",
    "    start=start,\n",
    "    end=end,\n",
    "    plot_start=70203681,\n",
    "    plot_end=70255052,\n",
    "    track_crop=16,\n",
    "    track_pool=1,\n",
    "    save_suffix='_wt_highlight_tss4',\n",
    "    annotate_features=annotate_features,\n",
    "    track_scale=0.05,\n",
    "    track_clip=0.20,\n",
    "    track_scale_qtl=0.98,\n",
    "    highlight_area=True,\n",
    "    highlight_start=attention_box_start_x,\n",
    "    highlight_end=attention_box_end_x,\n",
    "    highlight_start_y=attention_box_start_y,\n",
    "    highlight_end_y=attention_box_end_y,\n",
    "    save_figs=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "borzoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
