train:
  batch_size: 32
  patience: 20
  # agc_clip: 0.1
  num_gpu: 1
  train_epochs_max: 100
  loss: "mse" # default poisson. Options: mse; bce; poisson_mn; poisson_kl; mse_udot; poisson。
  optimizer: "adam" # default sgd. Options: adam; adamw; sgd; momentum.
  learning_rate: 0.0001 # default 0.01.
  adam_beta1: 0.9 # default 0.9.
  adam_beta2: 0.999 # default 0.999.
  decay_rate: 0.98 # default 0.96.

model:
  num_bins: 1024
  num_tracks: 106
  activation: "gelu"
  norm_type: "batch"
  bn_momentum: 0.9
  kernel_initializer: "lecun_normal"
  l2_scale: 2.0e-8
  trunk:
    - block_name: "conv_dna"
      filters: 256
      kernel_size: 15
      norm_type: "null"
      activation: "linear"
      pool_size: 2
    - block_name: "res_tower"
      filters_init: 312
      filters_end: 512
      divisible_by: 32
      kernel_size: 15
      num_convs: 1
      pool_size: 2
      repeat: 4
    - block_name: "transformer"
      heads: 8
      key_size: 64 # The size of each key and query embedding per head.
      dropout: 0
      dense_expansion: 0
      mha_l2_scale: 0 # default 0, 0意味着对权重不使用惩罚项。 
      mha_initializer: "he_normal" # 使用正态分布初始化权重，适合ReLU 激活函数
      attention_dropout: 0
      position_dropout: 0
    - block_name: "global_pool"
      pool_type: "attention"
      pool_axis: 1
  head_rice:
    - block_name: "dense_head"
      # hidden_units: 128 # default None. If setting, will add a dense layer with this number of units before the final output layer.
      units: 23 # number of targets
      activation: "softplus"

  